name: Coop eCoop scrape + upsert

on:
  workflow_dispatch:
  schedule:
    - cron: "17 */12 * * *"   # twice a day; tweak as you like

concurrency:
  group: coop-ecoop-scrape
  cancel-in-progress: false

jobs:
  scrape:
    runs-on: ubuntu-latest
    timeout-minutes: 60
    strategy:
      fail-fast: false
      matrix:
        include:
          - store_name: "haapsalu eCoop"
            store_id: 445
            store_url: "https://coophaapsalu.ee"
            cats_file: "data/coop_haapsalu_categories.txt"
            out_csv: "out/ecoop_haapsalu.csv"
          - store_name: "vandra eCoop"
            store_id: 446
            store_url: "https://vandra.ecoop.ee"
            cats_file: "data/coop_vandra_categories.txt"
            out_csv: "out/ecoop_vandra.csv"

    env:
      DATABASE_URL: ${{ secrets.RW_DATABASE_URL }}
      PYTHONUNBUFFERED: "1"

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install crawler deps (pip + Playwright)
        run: |
          python -m pip install -U pip wheel
          pip install -U playwright requests selectolax lxml bs4 pandas asyncpg tenacity
          python -m playwright install --with-deps chromium

      - name: Run eCoop crawler (${ { matrix.store_name } })
        run: |
          mkdir -p out
          python3 scripts/ecoop_crawler.py \
            --store-url "${{ matrix.store_url }}" \
            --store-id  "${{ matrix.store_id }}" \
            --categories-file "${{ matrix.cats_file }}" \
            --upsert-db main \
            --headless true \
            --out "${{ matrix.out_csv }}"

      - name: Upload CSV artifact (for debugging)
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: ${{ matrix.out_csv }}
          path: ${{ matrix.out_csv }}
