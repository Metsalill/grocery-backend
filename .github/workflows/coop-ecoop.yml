name: Coop eCoop scrape + upsert

on:
  workflow_dispatch:
  schedule:
    - cron: "22 */12 * * *"  # twice per day; adjust as you like

concurrency:
  group: coop-ecoop
  cancel-in-progress: true

jobs:
  scrape:
    runs-on: ubuntu-latest
    timeout-minutes: 30

    strategy:
      fail-fast: false
      matrix:
        include:
          - store_id: 445
            store_name: "haapsalu eCoop"
            cats_file: data/coop_haapsalu_categories.txt
            out_name: ecoop_haapsalu.csv
          - store_id: 446
            store_name: "vandra eCoop"
            cats_file: data/coop_vandra_categories.txt
            out_name: ecoop_vandra.csv

    env:
      TZ: Europe/Tallinn
      DATABASE_URL: ${{ secrets.RW_DATABASE_URL }}
      PYTHONUNBUFFERED: "1"
      # the crawler reads these from env (it does not accept --store-id/--upsert-db)
      STORE_ID: ${{ matrix.store_id }}
      UPSERT_DB: "main"   # set to "none" to disable ingest and only write CSV

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install crawler deps (pip + Playwright)
        run: |
          python -m pip install --upgrade pip
          # core libs the crawler uses
          pip install playwright asyncpg httpx pydantic tenacity selectolax bs4 lxml
          # install browser runtime
          python -m playwright install --with-deps chromium

      - name: Run eCoop crawler (${{ matrix.store_name }})
        run: |
          mkdir -p out
          python scripts/ecoop_crawler.py \
            --categories-file "${{ matrix.cats_file }}" \
            --page-limit 0 \
            --max-products 0 \
            --headless 1 \
            --out "out/${{ matrix.out_name }}"

      - name: Upload CSV artifact (${{ matrix.store_name }})
        uses: actions/upload-artifact@v4
        with:
          name: ${{ matrix.out_name }}
          path: out/${{ matrix.out_name }}
