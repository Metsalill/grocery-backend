name: Coop eCoop scrape + upsert

on:
  # Run on a schedule (adjust to your liking) and on manual dispatch
  schedule:
    - cron: "15 */6 * * *"
  workflow_dispatch: {}

concurrency:
  group: ecoop-scrape
  cancel-in-progress: false

jobs:
  scrape:
    runs-on: ubuntu-latest
    timeout-minutes: 90   # previously 30m â†’ increase to avoid mid-run cancellations
    strategy:
      fail-fast: false
      matrix:
        # Two eCoop stores + 4 category shards each (increase to 6 if still tight)
        store:
          - store_id: "445"
            store_name: "haapsalu eCoop"
            categories: "data/coop_haapsalu_categories.txt"
            out: "ecoop_haapsalu"
          - store_id: "446"
            store_name: "vandra eCoop"
            categories: "data/coop_vandra_categories.txt"
            out: "ecoop_vandra"
        cat_index: [0, 1, 2, 3]
    env:
      TZ: Europe/Tallinn
      PYTHONUNBUFFERED: "1"
      DATABASE_URL: ${{ secrets.RW_DATABASE_URL }}
      CAT_SHARDS: "4"   # keep in sync with matrix.cat_index length

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"
          cache: "pip"

      # Caches Playwright browsers between shards/runs (big speed-up)
      - name: Cache Playwright browsers
        uses: actions/cache@v4
        with:
          path: ${{ runner.temp }}/pw-browsers
          key: pw-${{ runner.os }}-chromium

      - name: Install crawler deps (pip + Playwright)
        env:
          PLAYWRIGHT_BROWSERS_PATH: ${{ runner.temp }}/pw-browsers
        run: |
          pip install -r requirements.txt || true
          # Ensure these are present for DB + crawling
          pip install asyncpg playwright
          python -m playwright install --with-deps chromium

      - name: Run eCoop crawler (${{ matrix.store.store_name }} | shard ${{ matrix.cat_index }})
        env:
          PLAYWRIGHT_BROWSERS_PATH: ${{ runner.temp }}/pw-browsers
          STORE_ID: ${{ matrix.store.store_id }}   # many crawlers read STORE_ID from env
        run: |
          mkdir -p out
          python3 scripts/ecoop_crawler.py \
            --categories-file "${{ matrix.store.categories }}" \
            --cat-shards "${CAT_SHARDS}" \
            --cat-index "${{ matrix.cat_index }}" \
            --req-delay 0.25 \
            --nav-timeout 60 \
            --category-timeout 420 \
            --goto-strategy auto \
            --out "out/${{ matrix.store.out }}_${{ matrix.cat_index }}.csv" \
            --write-empty-csv

      # Optional: upload per-shard CSVs for debugging/diffing (ingest happens inside the crawler)
      - name: Upload CSV artifact (per shard)
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: ${{ matrix.store.out }}_${{ matrix.cat_index }}
          path: out/${{ matrix.store.out }}_${{ matrix.cat_index }}.csv
          if-no-files-found: ignore
