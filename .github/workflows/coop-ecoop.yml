name: Coop eCoop scrape + upsert

on:
  workflow_dispatch: {}
  schedule:
    # every 6 hours UTC
    - cron: "0 */6 * * *"

jobs:
  scrape:
    runs-on: ubuntu-latest
    timeout-minutes: 30

    strategy:
      fail-fast: false
      matrix:
        include:
          - store_id: 445
            store_host: "https://coophaapsalu.ee"
            cats_file: "data/coop_haapsalu_categories.txt"
            out_csv: "out/ecoop_haapsalu.csv"

          - store_id: 446
            store_host: "https://vandra.ecoop.ee"
            cats_file: "data/coop_vandra_categories.txt"
            out_csv: "out/ecoop_vandra.csv"

    env:
      PYTHONUNBUFFERED: "1"
      DATABASE_URL: ${{ secrets.RW_DATABASE_URL }}   # Railway RW DB URL
      STORE_ID: ${{ matrix.store_id }}               # read by the crawler

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install deps
        run: |
          python -m pip install --upgrade pip
          # deps used across our crawlers
          pip install requests httpx selectolax lxml pandas tenacity tqdm

      - name: Run eCoop crawler
        run: |
          mkdir -p out
          python3 scripts/ecoop_crawler.py \
            --categories-file "${{ matrix.cats_file }}" \
            --out "${{ matrix.out_csv }}" \
            --page-limit 500 \
            --max-products 50000 \
            --req-delay 0.4

      - name: Upload CSV artifact (for debugging / diffing)
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: ${{ matrix.store_id }}-ecoop-products
          path: ${{ matrix.out_csv }}
          if-no-files-found: ignore
