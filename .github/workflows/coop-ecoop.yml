name: Coop eCoop scrape + upsert

on:
  schedule:
    - cron: "17 3 * * *"   # run daily, adjust as you like
  workflow_dispatch:

jobs:
  scrape:
    name: scrape ({{ matrix.store_name }}, {{ matrix.city }}, shard {{ matrix.cat_index }})
    runs-on: ubuntu-latest
    timeout-minutes: 120            # whole job (each shard)
    strategy:
      fail-fast: false
      matrix:
        cat_index: [0, 1, 2, 3, 4, 5, 6, 7]   # 8 shards -> guarantees partial uploads
        include:
          - store_name: haapsalu eCoop
            city: haapsalu
            store_id: 445
            store_host: coophaapsalu.ee
            cats: data/coop_haapsalu_categories.txt
          - store_name: vandra eCoop
            city: vandra
            store_id: 446
            store_host: vandra.ecoop.ee
            cats: data/coop_vandra_categories.txt

    concurrency:
      group: coop-ecoop-${{ matrix.city }}-${{ matrix.cat_index }}
      cancel-in-progress: false

    steps:
      - name: Checkout repo
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install crawler deps (pip + Playwright)
        run: |
          python -m pip install -U pip
          pip install -r requirements.txt
          python -m playwright install --with-deps chromium

      - name: Run eCoop crawler (${{ matrix.store_name }} | shard ${{ matrix.cat_index }})
        id: crawl
        timeout-minutes: 40                     # cap each shard; ensures we upload before any job limit
        env:
          PYTHONUNBUFFERED: "1"
        run: |
          mkdir -p out
          OUT="out/ecoop_${{ matrix.city }}_${{ matrix.cat_index }}.csv"
          python3 scripts/ecoop_crawler.py \
            --store-url https://${{ matrix.store_host }} \
            --store-host ${{ matrix.store_host }} \
            --store-id ${{ matrix.store_id }} \
            --categories-file ${{ matrix.cats }} \
            --cat-shards 8 \
            --cat-index  ${{ matrix.cat_index }} \
            --page-limit 9999 \
            --max-products 1000000 \
            --req-delay 0.2 \
            --pdp-workers 2 \
            --goto-strategy networkidle \
            --headless 1 \
            --out "$OUT" \
            --upsert-db main \
            || echo "[warn] crawler returned non-zero; uploading whatever was written"

      # IMPORTANT: artifact "name" must NOT contain a slash, only the "path" can.
      - name: Upload CSV artifact (per shard)
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: ecoop_${{ matrix.city }}_${{ matrix.cat_index }}
          path: out/ecoop_${{ matrix.city }}_${{ matrix.cat_index }}.csv
          if-no-files-found: ignore
          retention-days: 7
