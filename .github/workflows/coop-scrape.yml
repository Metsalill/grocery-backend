name: "Coop – Category Crawl (Playwright) + optional DB upsert"

on:
  workflow_dispatch:
    inputs:
      region:
        description: "Base region URL (e.g., https://coophaapsalu.ee or https://vandra.ecoop.ee)"
        required: false
        default: "https://vandra.ecoop.ee"
      categories_multiline:
        description: "Category URLs/paths (one per line). e.g. /et/tooted/53-joogid"
        required: false
        default: ""
      categories_file_url:
        description: "Optional raw URL to a .txt with categories (one per line)"
        required: false
        default: ""
      page_limit:
        description: "Discovery cap per category (0 = all)"
        required: false
        default: "0"
      max_products:
        description: "Post-discovery cap per category (0 = all)"
        required: false
        default: "0"
      headless:
        description: "Chromium headless (1/0)"
        required: false
        default: "1"
      req_delay:
        description: "Delay between actions (seconds)"
        required: false
        default: "0.5"
      pdp_workers:
        description: "Concurrent PDP tabs per category"
        required: false
        default: "4"
      shards:
        description: "Parallel shards for categories (1–8)."
        required: false
        default: "1"
      upsert_db:
        description: "Set to 1 to upsert into public.staging_coop_products"
        required: false
        default: "0"

  # 05:30 Tallinn daily (02:30 UTC)
  schedule:
    - cron: "30 2 * * *"

concurrency:
  group: coop-scrape-${{ github.ref }}-${{ github.event_name }}
  cancel-in-progress: false

jobs:
  # Build a matrix with exactly N shards
  prepare-matrix:
    runs-on: ubuntu-latest
    outputs:
      matrix: ${{ steps.mk.outputs.matrix }}
      region_url: ${{ steps.vars.outputs.region_url }}
      cats_multiline: ${{ steps.vars.outputs.cats_multiline }}
      cats_file_url: ${{ steps.vars.outputs.cats_file_url }}
      page_limit: ${{ steps.vars.outputs.page_limit }}
      max_products: ${{ steps.vars.outputs.max_products }}
      headless: ${{ steps.vars.outputs.headless }}
      req_delay: ${{ steps.vars.outputs.req_delay }}
      pdp_workers: ${{ steps.vars.outputs.pdp_workers }}
      upsert_flag: ${{ steps.vars.outputs.upsert_flag }}
      shards: ${{ steps.vars.outputs.shards }}
    steps:
      - id: vars
        run: |
          echo "region_url=${{ inputs.region }}" >> "$GITHUB_OUTPUT"
          echo "cats_multiline=${{ inputs.categories_multiline }}" >> "$GITHUB_OUTPUT"
          echo "cats_file_url=${{ inputs.categories_file_url }}" >> "$GITHUB_OUTPUT"
          echo "page_limit=${{ inputs.page_limit }}" >> "$GITHUB_OUTPUT"
          echo "max_products=${{ inputs.max_products }}" >> "$GITHUB_OUTPUT"
          echo "headless=${{ inputs.headless }}" >> "$GITHUB_OUTPUT"
          echo "req_delay=${{ inputs.req_delay }}" >> "$GITHUB_OUTPUT"
          echo "pdp_workers=${{ inputs.pdp_workers }}" >> "$GITHUB_OUTPUT"
          echo "upsert_flag=${{ inputs.upsert_db }}" >> "$GITHUB_OUTPUT"
          # clamp shards to [1,8]
          N="${{ inputs.shards }}"
          if [ -z "$N" ] || [ "$N" -lt 1 ]; then N=1; fi
          if [ "$N" -gt 8 ]; then N=8; fi
          echo "shards=$N" >> "$GITHUB_OUTPUT"
      - id: mk
        env:
          N: ${{ steps.vars.outputs.shards }}
        run: |
          arr='['
          for ((i=0;i<${N};i++)); do
            arr="${arr}${i},"
          done
          arr="${arr%,}]"
          echo "matrix={\"shard\":${arr}}" >> "$GITHUB_OUTPUT"

  crawl:
    needs: prepare-matrix
    runs-on: ubuntu-latest
    strategy:
      fail-fast: false
      matrix: ${{ fromJSON(needs.prepare-matrix.outputs.matrix) }}

    env:
      REGION_URL:     ${{ needs.prepare-matrix.outputs.region_url }}
      CATS_MULTILINE: ${{ needs.prepare-matrix.outputs.cats_multiline }}
      CATS_FILE_URL:  ${{ needs.prepare-matrix.outputs.cats_file_url }}
      PAGE_LIMIT:     ${{ needs.prepare-matrix.outputs.page_limit }}
      MAX_PRODUCTS:   ${{ needs.prepare-matrix.outputs.max_products }}
      HEADLESS:       ${{ needs.prepare-matrix.outputs.headless }}
      REQ_DELAY:      ${{ needs.prepare-matrix.outputs.req_delay }}
      PDP_WORKERS:    ${{ needs.prepare-matrix.outputs.pdp_workers }}
      COOP_UPSERT_DB: ${{ needs.prepare-matrix.outputs.upsert_flag }}
      SHARD_COUNT:    ${{ needs.prepare-matrix.outputs.shards }}
      SHARD_INDEX:    ${{ matrix.shard }}

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install Python deps & Playwright
        run: |
          python -m pip install --upgrade pip
          pip install playwright==1.46.0 asyncpg
          python -m playwright install --with-deps chromium

      - name: (Optional) Download categories file
        if: ${{ env.CATS_FILE_URL != '' }}
        run: |
          curl -fsSL "$CATS_FILE_URL" -o coop_categories.txt

      - name: Choose DB URL (prefer PUBLIC)
        if: ${{ env.COOP_UPSERT_DB == '1' }}
        run: |
          if [ -n "${{ secrets.DATABASE_URL_PUBLIC }}" ]; then
            echo "DATABASE_URL=${{ secrets.DATABASE_URL_PUBLIC }}" >> "$GITHUB_ENV"
            echo "[db] Using DATABASE_URL_PUBLIC"
          elif [ -n "${{ secrets.DATABASE_URL }}" ]; then
            echo "DATABASE_URL=${{ secrets.DATABASE_URL }}" >> "$GITHUB_ENV"
            echo "[db] Using DATABASE_URL"
          else
            echo "[db] Warning: No DB URL provided; script will skip upsert."
          fi

      - name: Run Coop crawler (shard ${{ env.SHARD_INDEX }} of ${{ env.SHARD_COUNT }})
        shell: bash
        run: |
          set -euo pipefail
          mkdir -p out

          # Build categories master file (UI URL > multiline > repo defaults > tiny fallback)
          CATS_ALL="coop_categories.txt"
          : > "$CATS_ALL"

          if [ -s "coop_categories.txt" ]; then
            echo ">> using categories downloaded from URL"
          elif [ -n "$CATS_MULTILINE" ]; then
            printf '%s\n' "$CATS_MULTILINE" > "$CATS_ALL"
          else
            if echo "$REGION_URL" | grep -qi 'vandra\.ecoop\.ee'; then
              REPO_FILE="data/coop_vandra_categories.txt"
            elif echo "$REGION_URL" | grep -qi 'coophaapsalu\.ee'; then
              REPO_FILE="data/coop_haapsalu_categories.txt"
            else
              REPO_FILE=""
            fi
            if [ -n "$REPO_FILE" ] && [ -f "$REPO_FILE" ]; then
              echo ">> using repo categories file: $REPO_FILE"
              cp "$REPO_FILE" "$CATS_ALL"
            fi
          fi

          if [ ! -s "$CATS_ALL" ]; then
            if echo "$REGION_URL" | grep -qi 'coophaapsalu\.ee'; then
              echo "/tootekategooria/joogid/" > "$CATS_ALL"
            else
              echo "/et/tooted/53-joogid" > "$CATS_ALL"
            fi
          fi

          # CRLF → LF
          sed -e 's/\r$//' -i "$CATS_ALL"

          # Slice for this shard
          CATS_SHARD="coop_categories_shard.txt"
          awk -v n="${SHARD_COUNT}" -v i="${SHARD_INDEX}" 'NF{ if ((NR-1)%n==i) print }' "$CATS_ALL" > "$CATS_SHARD"
          echo "=== Categories for shard ${SHARD_INDEX}/${SHARD_COUNT} ==="
          cat "$CATS_SHARD" || true
          echo "==============================================="

          # Run crawler
          python scripts/coop_crawl_categories_pw.py \
            --region "$REGION_URL" \
            --categories-multiline "" \
            --categories-file "$CATS_SHARD" \
            --page-limit "$PAGE_LIMIT" \
            --max-products "$MAX_PRODUCTS" \
            --headless "$HEADLESS" \
            --req-delay "$REQ_DELAY" \
            --pdp-workers "$PDP_WORKERS" \
            --out "out/coop_products_${{ github.run_id }}_shard${SHARD_INDEX}.csv"

      - name: Upload CSV artifact
        uses: actions/upload-artifact@v4
        with:
          name: coop-products-${{ github.run_id }}
          path: out/*.csv
          if-no-files-found: ignore
          retention-days: 7
