name: "Rimi → Category Crawl (Playwright) + optional DB upsert"

on:
  workflow_dispatch:
    inputs:
      categories_multiline:
        description: "Paste Rimi category URLs (one per line). Example: https://www.rimi.ee/epood/ee/tooted/leivad-saiad-kondiitritooted"
        required: false
        default: ""
      page_limit:
        description: "Max pages per category (0 = all)"
        required: false
        default: "0"
      max_products:
        description: "Hard cap on total PDPs (0 = unlimited)"
        required: false
        default: "0"
      headless:
        description: "Headless browser (1/0)"
        required: false
        default: "1"
      req_delay:
        description: "Delay between page ops (sec, can be 0.0)"
        required: false
        default: "0.5"
  schedule:
    - cron: "19 2 * * *"

concurrency:
  group: ${{ github.event_name == 'workflow_dispatch' && format('rimi-crawl-{0}', github.run_id) || 'rimi-crawl' }}
  cancel-in-progress: true

jobs:
  crawl-and-upsert:
    name: crawl-and-upsert (shard ${{ matrix.shard }})
    runs-on: ubuntu-latest
    timeout-minutes: 90
    strategy:
      fail-fast: false
      matrix:
        shard: [0, 1]
    env:
      PYTHONUNBUFFERED: "1"
      OUTPUT_CSV: data/rimi_products.csv
      DATABASE_URL: ${{ secrets.DATABASE_URL_PUBLIC }}
      SHARD: ${{ matrix.shard }}

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Cache pip
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements-scraper.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-

      - name: Install deps (Playwright + libs)
        run: |
          set -euo pipefail
          pip install --upgrade pip
          pip install -r requirements-scraper.txt || pip install playwright beautifulsoup4 lxml pg8000 psycopg2-binary
          python -m playwright install --with-deps chromium

      - name: Verify scraper file exists
        shell: bash
        run: |
          set -euo pipefail
          test -f scripts/rimi_crawl_categories_pw.py || { echo "::error::scripts/rimi_crawl_categories_pw.py missing"; exit 1; }
          python -m py_compile scripts/rimi_crawl_categories_pw.py
          chmod +x scripts/rimi_crawl_categories_pw.py

      - name: Clean workspace (fresh data dir)
        shell: bash
        run: |
          set -euo pipefail
          rm -rf data
          mkdir -p data

      - name: Prepare category list (all → shard slice)
        shell: bash
        env:
          SHARDS: 2
        run: |
          set -euo pipefail
          if [[ -n "${{ github.event.inputs.categories_multiline }}" ]]; then
            printf "%s\n" "${{ github.event.inputs.categories_multiline }}" | tr -d '\r' | sed '/^[[:space:]]*$/d' > data/rimi_categories_all.txt
            echo "Using categories from manual input."
          elif [[ -f data/rimi_categories.txt ]]; then
            cp -f data/rimi_categories.txt data/rimi_categories_all.txt
            echo "Using repo file data/rimi_categories.txt"
          else
            printf '%s\n' 'https://www.rimi.ee/epood/ee/tooted/leivad-saiad-kondiitritooted' > data/rimi_categories_all.txt
            echo "Using built-in sample category."
          fi

          : "${SHARD:?SHARD env is required}"
          awk -v s="$SHARD" -v n="$SHARDS" 'NR>0 { if ((NR-1)%n==s) print }' \
            data/rimi_categories_all.txt > data/rimi_categories.txt

          echo "TOTAL categories: $(wc -l < data/rimi_categories_all.txt)"
          echo "Sharding: SHARD=$SHARD / SHARDS=$SHARDS"
          echo "Slice count: $(wc -l < data/rimi_categories.txt)"
          nl -ba data/rimi_categories.txt | sed -n '1,10p' || true

      - name: Crawl Rimi categories (Playwright)
        shell: bash
        env:
          REQ_DELAY: ${{ github.event.inputs.req_delay || '0.5' }}
          PAGE_LIMIT: ${{ github.event.inputs.page_limit || '0' }}
          MAX_PRODUCTS: ${{ github.event.inputs.max_products || '0' }}
          HEADLESS: ${{ github.event.inputs.headless || '1' }}
        run: |
          set -euo pipefail
          echo "::group::Runtime info"
          python --version
          echo "Start (UTC): $(date -u)"
          echo "::endgroup::"

          set +e
          set +o pipefail
          stdbuf -oL -eL timeout -k 45s 40m \
            python -u scripts/rimi_crawl_categories_pw.py \
              --cats-file data/rimi_categories.txt \
              --page-limit "${PAGE_LIMIT}" \
              --max-products "${MAX_PRODUCTS}" \
              --headless "${HEADLESS}" \
              --req-delay "${REQ_DELAY}" \
              --output-csv "${OUTPUT_CSV}" \
            > >(tee data/rimi_run.log) 2>data/rimi_run.err
          status=${PIPESTATUS[0]}
          set -o pipefail
          set -e

          echo "End (UTC): $(date -u) — crawler exit code: ${status}"
          echo "::group::Crawler outputs"
          ls -lah data || true
          [[ -f "${OUTPUT_CSV}" ]] && { echo "CSV size:"; wc -l "${OUTPUT_CSV}"; du -h "${OUTPUT_CSV}"; } || echo "CSV not found"
          [[ -f data/rimi_run.log ]] && { echo "Log size:"; du -h data/rimi_run.log; } || echo "Log not found"
          [[ -s data/rimi_run.err ]] && { echo "---- stderr tail ----"; tail -n 120 data/rimi_run.err; } || true
          echo "::endgroup::"

          if [ "${status}" -eq 124 ]; then
            echo "::warning::Crawler hit the time budget (timeout 40m). Continuing with partial results."
          elif [ "${status}" -ne 0 ]; then
            echo "::warning::Crawler exited non-zero (${status}); continuing"
          fi

      - name: Ensure CSV exists (write header if missing/empty)
        shell: bash
        run: |
          set -euo pipefail
          if [[ ! -s "${OUTPUT_CSV}" ]]; then
            echo "::warning::${OUTPUT_CSV} missing or empty — creating header-only CSV."
            python - <<'PY'
import csv, os
path = os.environ.get("OUTPUT_CSV","data/rimi_products.csv")
fields = [
  "store_chain","store_name","store_channel",
  "ext_id","ean_raw","sku_raw","name","size_text","brand","manufacturer",
  "price","currency","image_url","category_path","category_leaf","source_url",
]
os.makedirs(os.path.dirname(path) or ".", exist_ok=True)
with open(path, "w", newline="", encoding="utf-8") as f:
    csv.DictWriter(f, fieldnames=fields).writeheader()
print("Wrote header-only CSV ->", path)
PY
          fi

      - name: Dump crawler log (tail)
        if: always()
        run: |
          echo "----- last 200 lines of data/rimi_run.log -----"
          if [ -f data/rimi_run.log ]; then
            tail -n 200 data/rimi_run.log
          else
            echo "(no log found)"
          fi

      - name: Show CSV head (debug)
        shell: bash
        run: |
          set -euo pipefail
          ls -lah data || true
          echo "---- line count ----"
          wc -l "${OUTPUT_CSV}" || true
          echo "---- first 12 lines ----"
          head -n 12 "${OUTPUT_CSV}" || true

      - name: Upload crawl artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: rimi-crawl-shard-${{ matrix.shard }}-${{ github.run_id }}
          path: |
            data/rimi_products.csv
            data/rimi_run.log
            data/rimi_run.err
            data/rimi_debug/**/*.png
          if-no-files-found: warn
          retention-days: 7

      # -------- OPTIONAL DB PART (runs only if DATABASE_URL is set) --------
      - name: Install psql client
        if: ${{ env.DATABASE_URL != '' }}
        run: |
          sudo apt-get update
          sudo apt-get install -y postgresql-client

      - name: DB sanity (connection)
        if: ${{ env.DATABASE_URL != '' }}
        shell: bash
        run: |
          set -euo pipefail
          psql "$DATABASE_URL" -c "\conninfo" || true
          psql "$DATABASE_URL" -c "SELECT current_database(), current_user;" || true

      - name: Prepare DB (staging & helper tables)
        if: ${{ env.DATABASE_URL != '' }}
        shell: bash
        run: |
          set -euo pipefail
          psql "$DATABASE_URL" -v ON_ERROR_STOP=1 <<'SQL'
          BEGIN;
          CREATE TABLE IF NOT EXISTS public.staging_rimi_products (
            ext_id        text PRIMARY KEY,
            name          text NOT NULL,
            ean_raw       text,
            sku_raw       text,
            ean_norm      text GENERATED ALWAYS AS (regexp_replace(COALESCE(ean_raw,''), '[^0-9]', '', 'g')) STORED,
            size_text     text,
            brand         text,
            price         numeric(12,2),
            currency      text DEFAULT 'EUR',
            category_path text,
            category_leaf text,
            collected_at  timestamptz DEFAULT now()
          );
          CREATE EXTENSION IF NOT EXISTS pg_trgm;
          CREATE INDEX IF NOT EXISTS ix_rimi_ean       ON public.staging_rimi_products (ean_norm);
          CREATE INDEX IF NOT EXISTS ix_rimi_name_trgm ON public.staging_rimi_products USING gin (name gin_trgm_ops);

          INSERT INTO public.stores (name, chain, is_online)
          SELECT 'Rimi ePood', 'Rimi', TRUE
          WHERE NOT EXISTS (SELECT 1 FROM public.stores WHERE chain='Rimi' AND is_online=TRUE);

          CREATE TABLE IF NOT EXISTS public.rimi_candidates (
            ext_id        text PRIMARY KEY,
            ean_norm      text,
            ean_raw       text,
            sku_raw       text,
            name          text,
            size_text     text,
            brand         text,
            price         numeric(12,2),
            currency      text,
            category_path text,
            category_leaf text,
            last_seen     timestamptz DEFAULT now()
          );
          COMMIT;
          SQL

      - name: Load CSV → staging + candidates
        if: ${{ env.DATABASE_URL != '' }}
        shell: bash
        run: |
          set -euo pipefail
          if [[ ! -s "${OUTPUT_CSV}" ]]; then
            echo "No rows in ${OUTPUT_CSV} — skipping DB load."
            exit 0
          fi

          cat > /tmp/rimi_load.sql <<'SQL'
          \set ON_ERROR_STOP on
          BEGIN;

          -- Full CSV mirror (matches file header)
          CREATE TEMP TABLE tmp_rimi_csv_full (
            store_chain   text,
            store_name    text,
            store_channel text,
            ext_id        text,
            ean_raw       text,
            sku_raw       text,
            name          text,
            size_text     text,
            brand         text,
            manufacturer  text,
            price         text,
            currency      text,
            image_url     text,
            category_path text,
            category_leaf text,
            source_url    text
          );

          \copy tmp_rimi_csv_full FROM :'csv' CSV HEADER

          -- Narrow to staging shape + cast price
          CREATE TEMP TABLE tmp_staging_rimi_products (
            ext_id        text PRIMARY KEY,
            name          text NOT NULL,
            ean_raw       text,
            sku_raw       text,
            ean_norm      text GENERATED ALWAYS AS (regexp_replace(COALESCE(ean_raw,''), '[^0-9]+', '', 'g')) STORED,
            size_text     text,
            brand         text,
            price         numeric(12,2),
            currency      text DEFAULT 'EUR',
            category_path text,
            category_leaf text,
            collected_at  timestamptz DEFAULT now()
          );

          INSERT INTO tmp_staging_rimi_products
            (ext_id,name,ean_raw,sku_raw,size_text,brand,price,currency,category_path,category_leaf)
          SELECT
            ext_id,
            name,
            ean_raw,
            sku_raw,
            size_text,
            brand,
            NULLIF(price,'')::numeric,
            currency,
            category_path,
            category_leaf
          FROM tmp_rimi_csv_full
          WHERE COALESCE(ext_id,'') <> '';

          -- Upsert into persistent staging table
          INSERT INTO public.staging_rimi_products
            (ext_id,name,ean_raw,sku_raw,size_text,brand,price,currency,category_path,category_leaf,collected_at)
          SELECT ext_id,name,ean_raw,sku_raw,size_text,brand,price,currency,category_path,category_leaf,now()
          FROM tmp_staging_rimi_products
          ON CONFLICT (ext_id) DO UPDATE
            SET name          = EXCLUDED.name,
                ean_raw       = EXCLUDED.ean_raw,
                sku_raw       = EXCLUDED.sku_raw,
                size_text     = COALESCE(EXCLUDED.size_text, public.staging_rimi_products.size_text),
                brand         = COALESCE(EXCLUDED.brand, public.staging_rimi_products.brand),
                price         = EXCLUDED.price,
                currency      = EXCLUDED.currency,
                category_path = COALESCE(EXCLUDED.category_path, public.staging_rimi_products.category_path),
                category_leaf = COALESCE(EXCLUDED.category_leaf, public.staging_rimi_products.category_leaf),
                collected_at  = now();

          -- Candidate pool (for later EAN adoption)
          INSERT INTO public.rimi_candidates
            (ext_id, ean_norm, ean_raw, sku_raw, name, size_text, brand, price, currency, category_path, category_leaf, last_seen)
          SELECT
            s.ext_id, s.ean_norm, s.ean_raw, s.sku_raw, s.name, s.size_text, s.brand, s.price, s.currency, s.category_path, s.category_leaf, now()
          FROM tmp_staging_rimi_products s
          ON CONFLICT (ext_id) DO UPDATE
            SET price         = EXCLUDED.price,
                currency      = EXCLUDED.currency,
                size_text     = COALESCE(EXCLUDED.size_text, public.rimi_candidates.size_text),
                brand         = COALESCE(EXCLUDED.brand, public.rimi_candidates.brand),
                category_path = COALESCE(EXCLUDED.category_path, public.rimi_candidates.category_path),
                category_leaf = COALESCE(EXCLUDED.category_leaf, public.rimi_candidates.category_leaf),
                ean_raw       = COALESCE(EXCLUDED.ean_raw, public.rimi_candidates.ean_raw),
                sku_raw       = COALESCE(EXCLUDED.sku_raw, public.rimi_candidates.sku_raw),
                last_seen     = now();

          COMMIT;
          SQL

          psql "$DATABASE_URL" -v csv="'${OUTPUT_CSV}'" -f /tmp/rimi_load.sql
