name: Rimi EAN probe (sample PDPs)

on:
  workflow_dispatch:
    inputs:
      urls_multiline:
        description: "Paste Rimi product URLs (one per line). If empty, uses data/rimi_urls.txt from repo."
        required: false
        default: ""
  schedule:
    # Run nightly as a small sanity check (uses data/rimi_urls.txt if present)
    - cron: "30 2 * * *"

concurrency:
  group: rimi-probe-eans
  cancel-in-progress: true

jobs:
  probe:
    runs-on: ubuntu-latest
    timeout-minutes: 30
    env:
      PYTHONUNBUFFERED: "1"

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"
          cache: "pip"

      - name: Install deps (Playwright + parsers)
        run: |
          set -euo pipefail
          python -V
          pip install --upgrade pip
          pip install playwright beautifulsoup4 lxml
          python -m playwright install --with-deps chromium

      - name: Create probe script (inline)
        shell: bash
        run: |
          mkdir -p scripts
          cat > scripts/rimi_probe_ean.py <<'PY'
          import asyncio, json, re, sys, csv, time
          from pathlib import Path
          from typing import Any, Dict, List, Tuple, Optional
          from bs4 import BeautifulSoup
          from playwright.async_api import async_playwright, TimeoutError as PWTimeout

          EAN_RE = re.compile(r'\b\d{13}\b')
          EAN_LABEL_RE = re.compile(r'\b(ean|gtin|gtin13|barcode|triipkood)\b', re.I)
          SKU_KEYS = {"sku","mpn","itemNumber","productCode","code"}
          EAN_KEYS = {"ean","ean13","gtin","gtin13","barcode"}

          def deep_find_kv(obj: Any, keys: set) -> Dict[str, str]:
              found = {}
              def walk(x):
                  if isinstance(x, dict):
                      for k, v in x.items():
                          lk = str(k).lower()
                          if lk in keys and isinstance(v, (str,int)):
                              found[lk] = str(v)
                          walk(v)
                  elif isinstance(x, list):
                      for i in x: walk(i)
              walk(obj)
              return found

          def parse_jsonld(soup: BeautifulSoup):
              flags, ean, sku = [], None, None
              for tag in soup.find_all("script", {"type": "application/ld+json"}):
                  try:
                      data = json.loads(tag.text)
                  except Exception:
                      continue
                  seq = data if isinstance(data, list) else [data]
                  for d in seq:
                      got = deep_find_kv(d, { *EAN_KEYS, *SKU_KEYS })
                      ean = ean or got.get("gtin13") or got.get("ean") or got.get("ean13") or got.get("barcode") or got.get("gtin")
                      sku = sku or got.get("sku") or got.get("mpn")
              if ean or sku: flags.append("jsonld")
              return ean, sku, flags

          def parse_microdata_meta(soup: BeautifulSoup):
              flags, ean, sku = [], None, None
              for ip in ("gtin13","gtin","ean","ean13","barcode"):
                  meta = soup.find(attrs={"itemprop": ip})
                  if meta:
                      ean = ean or (meta.get("content") or meta.get_text(strip=True))
              for ip in ("sku","mpn"):
                  meta = soup.find(attrs={"itemprop": ip})
                  if meta:
                      sku = sku or (meta.get("content") or meta.get_text(strip=True))
              if ean or sku: flags.append("microdata")
              return ean, sku, flags

          def parse_visible_text_for_ean(soup: BeautifulSoup):
              flags = []
              for el in soup.find_all(text=EAN_LABEL_RE):
                  seg = el.parent.get_text(" ", strip=True) if el and el.parent else str(el)
                  m = EAN_RE.search(seg)
                  if m:
                      flags.append("visible")
                      return m.group(0), flags
              m = EAN_RE.search(soup.get_text(" ", strip=True))
              if m:
                  flags.append("visible_guess")
                  return m.group(0), flags
              return None, flags

          def parse_imgs_for_gtin(soup: BeautifulSoup):
              flags = []
              for img in soup.find_all("img"):
                  cand = " ".join(filter(None, [img.get("src",""), img.get("alt","")]))
                  m = EAN_RE.search(cand)
                  if m:
                      flags.append("img_url")
                      return m.group(0), flags
              return None, flags

          async def probe_url(pw, url: str, timeout_ms=25000) -> Dict[str, Any]:
              browser = await pw.chromium.launch(headless=True, args=["--no-sandbox"])
              ctx = await browser.new_context(locale="et-EE",
                                             user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124 Safari/537.36",
                                             viewport={"width": 1366, "height": 900})
              page = await ctx.new_page()
              sniffed: List[Dict[str,Any]] = []
              async def on_response(resp):
                  try:
                      ct = resp.headers.get("content-type","")
                      if "application/json" in ct:
                          data = await resp.json()
                          sniffed.append({"url": resp.url, "data": data})
                  except Exception:
                      pass
              page.on("response", on_response)

              name = ean = sku = None
              sources = []
              try:
                  await page.goto(url, timeout=timeout_ms, wait_until="domcontentloaded")
                  for label in ("Nõustun","Nõustu","Accept","Allow all","OK","Selge"):
                      try:
                          await page.get_by_role("button", name=re.compile(label, re.I)).click(timeout=1500); break
                      except Exception: pass
                  await page.wait_for_timeout(1200)
                  html = await page.content()
                  soup = BeautifulSoup(html, "lxml")
                  h1 = soup.find("h1")
                  if h1: name = h1.get_text(strip=True)

                  e1, s1, f1 = parse_jsonld(soup); sources += f1; ean = ean or e1; sku = sku or s1
                  e2, s2, f2 = parse_microdata_meta(soup); sources += f2; ean = ean or e2; sku = sku or s2

                  for glb in ["__NUXT__","__NEXT_DATA__","APP_STATE","dataLayer"]:
                      try:
                          data = await page.evaluate(f"window['{glb}']")
                          if data:
                              got = deep_find_kv(data, { *EAN_KEYS, *SKU_KEYS })
                              ean = ean or got.get("gtin13") or got.get("ean") or got.get("ean13") or got.get("barcode") or got.get("gtin")
                              sku = sku or got.get("sku") or got.get("mpn") or got.get("code")
                              if got: sources.append(f"global:{glb}")
                      except Exception: pass

                  if not ean:
                      e3, f3 = parse_visible_text_for_ean(soup); sources += f3; ean = ean or e3
                  if not ean:
                      e4, f4 = parse_imgs_for_gtin(soup); sources += f4; ean = ean or e4

                  if not ean or not sku:
                      for rec in sniffed:
                          got = deep_find_kv(rec["data"], { *EAN_KEYS, *SKU_KEYS })
                          if got:
                              ean = ean or got.get("gtin13") or got.get("ean") or got.get("ean13") or got.get("barcode") or got.get("gtin")
                              sku = sku or got.get("sku") or got.get("mpn") or got.get("code")
                              sources.append("net")
              except PWTimeout:
                  sources.append("timeout")
              except Exception as e:
                  sources.append(f"err:{type(e).__name__}")
              finally:
                  await ctx.close(); await browser.close()

              return {"ext_id": url, "url": url, "name": name or "",
                      "ean_raw": ean or "", "sku_raw": sku or "",
                      "source_flags": ",".join(dict.fromkeys(sources))}

          async def main():
              urls: List[str] = []
              if len(sys.argv) > 1 and Path(sys.argv[1]).exists():
                  urls = [u.strip() for u in Path(sys.argv[1]).read_text().splitlines() if u.strip()]
              else:
                  print("No URL file provided.", file=sys.stderr); sys.exit(1)
              out_path = Path("rimi_probe_eans.csv")
              import csv
              with open(out_path, "w", newline="", encoding="utf-8") as f:
                  wr = csv.DictWriter(f, fieldnames=["ext_id","url","name","ean_raw","sku_raw","source_flags"])
                  wr.writeheader()
                  async with async_playwright() as pw:
                      for i, u in enumerate(urls, 1):
                          row = await probe_url(pw, u)
                          wr.writerow(row)
                          print(f"[{i}/{len(urls)}] {row['ean_raw'] or '—'}  {row['name'][:80]}")
                          time.sleep(0.6)
              print(f"Wrote {out_path}")
          if __name__ == "__main__":
              asyncio.run(main())
          PY

      - name: Prepare URL list
        shell: bash
        run: |
          set -euo pipefail
          mkdir -p data
          if [ -n "${{ github.event.inputs.urls_multiline }}" ]; then
            cat > data/rimi_urls.txt <<'LIST'
          ${{ github.event.inputs.urls_multiline }}
          LIST
          elif [ -f data/rimi_urls.txt ]; then
            echo "Using repo file data/rimi_urls.txt"
          else
            echo "::error::No URLs provided and data/rimi_urls.txt not found."; exit 1
          fi
          echo "Probing these URLs (first 20 shown):"
          nl -ba data/rimi_urls.txt | sed -n '1,20p'

      - name: Run probe
        shell: bash
        run: |
          set -euo pipefail
          # Keep a strict wall clock so the job never drifts
          timeout -k 15s 10m python scripts/rimi_probe_ean.py data/rimi_urls.txt

      - name: Summarize results
        shell: bash
        run: |
          python - <<'PY'
          import csv, os
          fn = "rimi_probe_eans.csv"
          t = e = s = 0
          with open(fn, encoding="utf-8") as f:
              r = csv.DictReader(f)
              rows = list(r)
          t = len(rows)
          e = sum(1 for x in rows if x["ean_raw"])
          s = sum(1 for x in rows if x["sku_raw"])
          print(f"Total: {t} | with EAN: {e} | with SKU: {s}")
          open(os.environ["GITHUB_STEP_SUMMARY"],"a").write(
              f"### Rimi EAN probe\n\n- Total URLs: **{t}**\n- With EAN: **{e}**\n- With SKU: **{s}**\n\nCSV artifact: `rimi_probe_eans.csv`\n")
          PY
          echo "Preview:"
          head -n 10 rimi_probe_eans.csv || true

      - name: Upload CSV artifact
        uses: actions/upload-artifact@v4
        with:
          name: rimi_probe_eans.csv
          path: rimi_probe_eans.csv
          if-no-files-found: error
          retention-days: 7
