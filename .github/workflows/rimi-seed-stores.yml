name: Seed Rimi physical stores
on:
  workflow_dispatch: {}

jobs:
  seed:
    runs-on: ubuntu-latest
    env:
      DATABASE_URL: ${{ secrets.DATABASE_URL_PUBLIC }}

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install scrape deps
        run: |
          set -euo pipefail
          pip install playwright bs4 lxml
          python -m playwright install --with-deps chromium

      - name: Scrape Rimi stores to CSV
        run: |
          set -euo pipefail
          mkdir -p data scripts
          # Write scraper
          cat > scripts/scrape_rimi_stores.py <<'PY'
import re, csv, time
from pathlib import Path
from bs4 import BeautifulSoup
from playwright.sync_api import sync_playwright

OUT = Path("data/rimi_stores.csv")
OUT.parent.mkdir(parents=True, exist_ok=True)

def extract_latlon_from_href(href: str):
    if not href:
        return None, None
    m = re.search(r'@(-?\d+\.\d+),(-?\d+\.\d+)', href)
    if m:
        return float(m.group(1)), float(m.group(2))
    m = re.search(r'query=(-?\d+\.\d+),(-?\d+\.\d+)', href)
    if m:
        return float(m.group(1)), float(m.group(2))
    return None, None

def main():
    url = "https://www.rimi.ee/kauplused"
    rows = []
    with sync_playwright() as p:
        browser = p.chromium.launch(headless=True)
        ctx = browser.new_context(locale="et-EE")
        page = ctx.new_page()
        page.goto(url, wait_until="networkidle")
        for _ in range(15):
            page.mouse.wheel(0, 3000)
            time.sleep(0.3)
        html = page.content()
        soup = BeautifulSoup(html, "lxml")
        cards = soup.select("a[href*='google'], div, li, article")
        seen = set()
        for card in cards:
            name = None
            for tag in card.select("strong, h1, h2, h3"):
                t = tag.get_text(" ", strip=True)
                if t and "rimi" in t.lower():
                    name = t
                    break
            if not name:
                continue
            text = card.get_text("\n", strip=True)
            address = None
            for line in text.splitlines():
                if any(ch.isdigit() for ch in line) and len(line) < 100 and "@" not in line:
                    address = line
                    break
            href = None
            a = card.select_one("a[href*='google']")
            if a:
                href = a.get("href")
            lat, lon = extract_latlon_from_href(href)
            if not name or not address:
                continue
            key = (name.strip(), address.strip())
            if key in seen:
                continue
            seen.add(key)
            rows.append((name.strip(), address.strip(), lat, lon, None))
        browser.close()
    dedup = {}
    for name, addr, lat, lon, ext in rows:
        key = (name, addr)
        if key not in dedup:
            dedup[key] = (name, addr, lat, lon, ext)
    with OUT.open("w", newline="", encoding="utf-8") as f:
        w = csv.writer(f)
        w.writerow(["name","address","lat","lon","external_key"])
        for name, addr, lat, lon, ext in sorted(dedup.values()):
            w.writerow([name, addr, "" if lat is None else lat, "" if lon is None else lon, ext or ""])
    print(f"Wrote {len(dedup)} rows -> {OUT}")

if __name__ == "__main__":
    main()
PY
          python scripts/scrape_rimi_stores.py
          echo "--- preview ---"
          head -n 10 data/rimi_stores.csv || true

      - name: Install psql
        run: |
          set -euo pipefail
          sudo apt-get update
          sudo apt-get install -y postgresql-client

      - name: Upsert stores and alias to Rimi ePood
        run: |
          set -euo pipefail
          # Write SQL to a temp file; backslash commands must be at column 0
          cat >/tmp/rimi_seed.sql <<'SQL'
\set ON_ERROR_STOP on

-- staging table
DROP TABLE IF EXISTS staging_rimi_stores;
CREATE TABLE staging_rimi_stores(
  name         text,
  address      text,
  lat          double precision,
  lon          double precision,
  external_key text
);

-- load CSV
\copy staging_rimi_stores(name,address,lat,lon,external_key)
  FROM 'data/rimi_stores.csv'
  WITH (FORMAT csv, HEADER true, NULL '');

-- upsert into stores (assumes unique on (chain,name) exists)
INSERT INTO stores (name, chain, address, lat, lon, latitude, longitude, is_online, external_key)
SELECT
  s.name, 'Rimi',
  NULLIF(s.address,''),
  s.lat, s.lon, s.lat, s.lon,
  false,
  NULLIF(s.external_key,'')
FROM staging_rimi_stores s
WHERE COALESCE(s.name,'') <> ''
ON CONFLICT (chain, name)
DO UPDATE SET
  address      = EXCLUDED.address,
  lat          = EXCLUDED.lat,
  lon          = EXCLUDED.lon,
  latitude     = EXCLUDED.latitude,
  longitude    = EXCLUDED.longitude,
  is_online    = false,
  external_key = COALESCE(EXCLUDED.external_key, stores.external_key);

-- alias physical Rimi stores to ePood id=440
CREATE TABLE IF NOT EXISTS store_price_source (
  store_id        bigint PRIMARY KEY,
  source_store_id bigint NOT NULL
);

INSERT INTO store_price_source (store_id, source_store_id)
SELECT st.id, 440
FROM stores st
WHERE st.chain='Rimi' AND COALESCE(st.is_online,false)=false
ON CONFLICT (store_id) DO UPDATE
SET source_store_id = EXCLUDED.source_store_id;

-- summary
\t on
SELECT 'staging_rows' AS what, COUNT(*) AS count FROM staging_rimi_stores
UNION ALL
SELECT 'physical_rimi_in_stores', COUNT(*) FROM stores WHERE chain='Rimi' AND COALESCE(is_online,false)=false
UNION ALL
SELECT 'aliased_to_440', COUNT(*) FROM store_price_source sps JOIN stores st ON st.id=sps.store_id
       WHERE st.chain='Rimi' AND COALESCE(st.is_online,false)=false AND sps.source_store_id=440;
\t off
SQL

          # Execute the SQL
          psql "$DATABASE_URL" -f /tmp/rimi_seed.sql
