name: "Barbora (Maxima EE) - Category Crawl + optional DB upsert"

on:
  workflow_dispatch:
    inputs:
      categories_multiline:
        description: "Category URLs (one per line). If empty, auto-discover from Barbora."
        required: false
        default: ""
      page_limit:
        description: "Max pages per category (0 = all)"
        required: false
        default: "0"
      max_products:
        description: "Cap total PDPs (0 = unlimited)"
        required: false
        default: "0"
      headless:
        description: "Headless (1/0)"
        required: false
        default: "1"
      req_delay:
        description: "Delay between steps (sec)"
        required: false
        default: "0.25"
      upsert_db:
        description: "Upsert into Postgres (1=yes, 0=just CSV)"
        required: false
        default: "1"
  schedule:
    - cron: "19 */3 * * *"

concurrency:
  group: barbora-category-crawl
  cancel-in-progress: true

jobs:
  crawl-and-upsert:
    runs-on: ubuntu-latest
    timeout-minutes: 110
    env:
      PYTHONUNBUFFERED: "1"
      OUTPUT_CSV: data/barbora_products.csv
      DATABASE_URL: ${{ secrets.DATABASE_URL_PUBLIC }}
      EXCLUDE_SECTIONS: "https://barbora.ee/kodukaubad-ja-vaba-aeg https://barbora.ee/enesehooldustooted https://barbora.ee/puhastustarbed-ja-lemmikloomatooted https://barbora.ee/lastekaubad"
      INCLUDE_SECTIONS: "https://barbora.ee/enesehooldustooted/suuhugieen/hambapastad https://barbora.ee/enesehooldustooted/raseerimisvahendid https://barbora.ee/enesehooldustooted/intiimhugieeni-vahendid https://barbora.ee/puhastustarbed-ja-lemmikloomatooted/pesupesemisvahendid https://barbora.ee/puhastustarbed-ja-lemmikloomatooted/noudepesuvahendid https://barbora.ee/puhastustarbed-ja-lemmikloomatooted/kodukeemia https://barbora.ee/puhastustarbed-ja-lemmikloomatooted/majapidamis-ja-koristustarbed https://barbora.ee/lastekaubad/piimasegud-ja-jatkupiimasegud https://barbora.ee/lastekaubad/pudrud https://barbora.ee/lastekaubad/mahkmed https://barbora.ee/lastekaubad/puuviljapureed https://barbora.ee/lastekaubad/laste-hugieenitarbed/niisked-salvratikud https://barbora.ee/lastekaubad/liha-ja-koogiviljapureed"

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Cache pip
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements-scraper.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-

      - name: Install deps (Playwright + libs)
        shell: bash
        run: |
          set -euo pipefail
          pip install -r requirements-scraper.txt || pip install playwright beautifulsoup4 lxml pg8000 psycopg2-binary
          python -m playwright install --with-deps chromium

      - name: Verify scraper exists
        shell: bash
        run: |
          set -euo pipefail
          if [ ! -f scripts/barbora_crawl_categories_pw.py ]; then
            echo "::error::scripts/barbora_crawl_categories_pw.py missing"
            exit 1
          fi
          python -m py_compile scripts/barbora_crawl_categories_pw.py
          chmod +x scripts/barbora_crawl_categories_pw.py

      - name: Prepare workspace
        shell: bash
        run: |
          set -euo pipefail
          rm -rf data
          mkdir -p data

      - name: Auto-discover categories (unless provided via input)
        env:
          INPUT_CATS: ${{ github.event.inputs.categories_multiline }}
          EXCLUDES: ${{ env.EXCLUDE_SECTIONS }}
          INCLUDES: ${{ env.INCLUDE_SECTIONS }}
        shell: bash
        run: |
          set -euo pipefail
          if [ -n "${INPUT_CATS}" ]; then
            printf "%s\n" "${INPUT_CATS}" | tr -d '\r' | sed '/^[[:space:]]*$/d' > data/barbora_categories.txt
            echo "Using categories from workflow input."
          else
            python - <<'PY'
import os
from urllib.parse import urljoin, urlparse
from playwright.sync_api import sync_playwright

BASE = "https://barbora.ee"
excludes = tuple(os.environ.get("EXCLUDES","").split())
includes = tuple(os.environ.get("INCLUDES","").split())
discovered = set()

def absu(href: str) -> str:
    return urljoin(BASE, href)

def is_category(url: str) -> bool:
    p = urlparse(url)
    if not (p.scheme and p.netloc and p.netloc.endswith("barbora.ee")):
        return False
    segs = [s for s in p.path.split("/") if s]
    if 1 <= len(segs) <= 3 and all(s not in ("retseptid","info","blogi") for s in segs):
        return True
    return False

def allowed(url: str) -> bool:
    if any(url.startswith(pref) for pref in includes):
        return True
    if any(url.startswith(pref) for pref in excludes):
        return False
    return is_category(url)

with sync_playwright() as pw:
    browser = pw.chromium.launch(headless=True)
    ctx = browser.new_context()
    page = ctx.new_page()
    page.goto(BASE, timeout=45000, wait_until="domcontentloaded")

    for a in page.locator("a").all():
        href = (a.get_attribute("href") or "").strip()
        if not href:
            continue
        u = absu(href)
        if allowed(u):
            discovered.add(u)

    os.makedirs("data", exist_ok=True)
    with open("data/barbora_categories.txt","w",encoding="utf-8") as f:
        for u in sorted(discovered):
            f.write(u+"\n")

    print(f"[discover] wrote {len(discovered)} categories -> data/barbora_categories.txt")
    ctx.close()
    browser.close()
PY
          fi
          echo "TOTAL categories: $(wc -l < data/barbora_categories.txt || echo 0)"
          echo "First 30 discovered (if any):"
          head -n 30 data/barbora_categories.txt || true

      - name: Build skip list from priced items (prices + candidates)
        if: ${{ env.DATABASE_URL != '' }}
        shell: bash
        run: |
          set -euo pipefail
          : > data/barbora_skip_ext_ids.txt
          psql "$DATABASE_URL" -At -v ON_ERROR_STOP=1 <<'SQL' > data/barbora_skip_ext_ids.txt
          WITH priced_from_prices AS (
            SELECT DISTINCT (regexp_match(p.source_url, '/p/([0-9]+)'))[1] AS ext_id
            FROM public.prices p
            JOIN public.stores s ON s.id = p.store_id
            WHERE s.chain='Maxima' AND s.name ILIKE '%Barbora%' AND s.is_online = TRUE
              AND p.source_url ~ '/p/[0-9]+'
          ),
          priced_from_candidates AS (
            SELECT DISTINCT ext_id::text
            FROM public.barbora_candidates
            WHERE COALESCE(price,0) > 0
              AND COALESCE(ext_id,'') <> ''
          )
          SELECT ext_id FROM priced_from_prices
          UNION
          SELECT ext_id FROM priced_from_candidates
          ORDER BY 1;
          SQL
          echo "Skip list rows (already priced): $(wc -l < data/barbora_skip_ext_ids.txt)" | tee data/_barbora_skip_count.txt

      - name: Crawl Barbora categories (Playwright)
        env:
          REQ_DELAY: ${{ github.event.inputs.req_delay }}
          PAGE_LIMIT: ${{ github.event.inputs.page_limit }}
          MAX_PRODUCTS: ${{ github.event.inputs.max_products }}
          HEADLESS: ${{ github.event.inputs.headless }}
          SKIP_EXT_FILE: data/barbora_skip_ext_ids.txt
        shell: bash
        run: |
          set -euo pipefail
          echo "::group::Runtime info"
          python --version
          echo "Start (UTC): $(date -u)"
          echo "::endgroup::"

          REQ_DELAY="${REQ_DELAY:-0.25}"
          PAGE_LIMIT="${PAGE_LIMIT:-0}"
          MAX_PRODUCTS="${MAX_PRODUCTS:-0}"
          HEADLESS="${HEADLESS:-1}"

          if [ -f "$SKIP_EXT_FILE" ]; then
            echo "SKIP_EXT_FILE present ($(wc -l < "$SKIP_EXT_FILE") rows)"
            SKIP_FLAG=(--skip-ext-file "$SKIP_EXT_FILE")
          else
            SKIP_FLAG=()
          fi

          set +e
          set +o pipefail
          stdbuf -oL -eL timeout -k 45s 80m \
            python -u scripts/barbora_crawl_categories_pw.py \
              --cats-file data/barbora_categories.txt \
              --page-limit "$PAGE_LIMIT" \
              --max-products "$MAX_PRODUCTS" \
              --headless "$HEADLESS" \
              --req-delay "$REQ_DELAY" \
              --output-csv "$OUTPUT_CSV" \
              "${SKIP_FLAG[@]}" \
            > >(tee data/barbora_run.log) 2>data/barbora_run.err
          status=${PIPESTATUS[0]}
          set -o pipefail
          set -e

          echo "End (UTC): $(date -u) - crawler exit code: ${status}"
          echo "::group::Crawler outputs"
          ls -lah data || true
          if [ -f "$OUTPUT_CSV" ]; then
            echo "CSV size:"; wc -l "$OUTPUT_CSV"; du -h "$OUTPUT_CSV"
          else
            echo "CSV not found"
          fi
          [ -f data/barbora_run.log ] && { echo "Log size:"; du -h data/barbora_run.log; } || echo "Log not found"
          [ -s data/barbora_run.err ] && { echo "---- stderr tail ----"; tail -n 120 data/barbora_run.err; } || true
          echo "::endgroup::"

          if [ "$status" -eq 124 ]; then
            echo "::warning::Crawler hit the time budget (timeout 80m). Continuing with partial results."
          elif [ "$status" -ne 0 ]; then
            echo "::warning::Crawler exited non-zero ($status); continuing"
          fi

      - name: Ensure CSV exists (header if empty)
        shell: bash
        run: |
          set -euo pipefail
          if [ ! -s "$OUTPUT_CSV" ]; then
            python - <<'PY'
import csv,os
p=os.environ.get('OUTPUT_CSV','data/barbora_products.csv')
os.makedirs(os.path.dirname(p) or '.', exist_ok=True)
flds=['store_chain','store_name','store_channel','ext_id','ean_raw','sku_raw','name','size_text','brand','manufacturer','price','currency','image_url','category_path','category_leaf','source_url']
with open(p,'w',newline='',encoding='utf-8') as f:
    csv.DictWriter(f, fieldnames=flds).writeheader()
print('Wrote header-only CSV ->', p)
PY
          fi

      - name: Upload crawl artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: barbora-crawl-${{ github.run_id }}
          path: |
            data/barbora_products.csv
            data/barbora_run.log
            data/barbora_run.err
            data/_barbora_skip_count.txt
          if-no-files-found: warn
          retention-days: 7

      - name: Install psql client
        if: ${{ env.DATABASE_URL != '' && github.event.inputs.upsert_db == '1' }}
        shell: bash
        run: |
          sudo apt-get update
          sudo apt-get install -y postgresql-client

      - name: DB sanity (connection)
        if: ${{ env.DATABASE_URL != '' && github.event.inputs.upsert_db == '1' }}
        shell: bash
        run: |
          set -euo pipefail
          psql "$DATABASE_URL" -c "\conninfo" || true
          psql "$DATABASE_URL" -c "SELECT current_database(), current_user;" || true

      - name: Prepare DB (store + staging)
        if: ${{ env.DATABASE_URL != '' && github.event.inputs.upsert_db == '1' }}
        shell: bash
        run: |
          psql "$DATABASE_URL" -v ON_ERROR_STOP=1 <<'SQL'
          BEGIN;
          INSERT INTO public.stores (name, chain, is_online)
          SELECT 'Barbora ePood', 'Maxima', TRUE
          WHERE NOT EXISTS (SELECT 1 FROM public.stores WHERE name='Barbora ePood' AND chain='Maxima' AND is_online=TRUE);

          CREATE TABLE IF NOT EXISTS public.staging_barbora_products (
            ext_id        text PRIMARY KEY,
            name          text NOT NULL,
            ean_raw       text,
            sku_raw       text,
            ean_norm      text GENERATED ALWAYS AS (regexp_replace(COALESCE(ean_raw,''), '[^0-9]', '', 'g')) STORED,
            size_text     text,
            brand         text,
            price         numeric(12,2),
            currency      text DEFAULT 'EUR',
            category_path text,
            category_leaf text,
            collected_at  timestamptz DEFAULT now()
          );
          CREATE EXTENSION IF NOT EXISTS pg_trgm;
          CREATE INDEX IF NOT EXISTS ix_barbora_ean       ON public.staging_barbora_products (ean_norm);
          CREATE INDEX IF NOT EXISTS ix_barbora_name_trgm ON public.staging_barbora_products USING gin (name gin_trgm_ops);

          CREATE TABLE IF NOT EXISTS public.barbora_candidates (
            ext_id        text PRIMARY KEY,
            ean_norm      text,
            ean_raw       text,
            sku_raw       text,
            name          text,
            size_text     text,
            brand         text,
            price         numeric(12,2),
            currency      text,
            category_path text,
            category_leaf text,
            last_seen     timestamptz DEFAULT now()
          );
          COMMIT;
          SQL

      - name: Load CSV -> staging + candidates
        if: ${{ env.DATABASE_URL != '' && github.event.inputs.upsert_db == '1' }}
        shell: bash
        run: |
          CSV_ABS="$(python -c 'import os;print(os.path.abspath("data/barbora_products.csv"))')"
          cat > /tmp/barbora_load.sql <<'SQL'
          \set ON_ERROR_STOP on
          BEGIN;

          CREATE TEMP TABLE tmp_barbora_csv_full (
            store_chain   text,
            store_name    text,
            store_channel text,
            ext_id        text,
            ean_raw       text,
            sku_raw       text,
            name          text,
            size_text     text,
            brand         text,
            manufacturer  text,
            price         text,
            currency      text,
            image_url     text,
            category_path text,
            category_leaf text,
            source_url    text
          );

          \copy tmp_barbora_csv_full FROM '__CSV__' CSV HEADER

          CREATE TEMP TABLE tmp_staging_barbora_products AS
          SELECT
            ext_id,
            name,
            ean_raw,
            sku_raw,
            regexp_replace(COALESCE(ean_raw,''),'[^0-9]+','','g') AS ean_norm,
            size_text,
            brand,
            NULLIF(price,'')::numeric AS price,
            NULLIF(currency,'')::text AS currency,
            category_path,
            category_leaf
          FROM tmp_barbora_csv_full
          WHERE COALESCE(ext_id,'') <> '';

          INSERT INTO public.staging_barbora_products
            (ext_id,name,ean_raw,sku_raw,size_text,brand,price,currency,category_path,category_leaf,collected_at)
          SELECT ext_id,name,ean_raw,sku_raw,size_text,brand,price,COALESCE(currency,'EUR'),category_path,category_leaf,now()
          FROM tmp_staging_barbora_products
          ON CONFLICT (ext_id) DO UPDATE
            SET name          = EXCLUDED.name,
                ean_raw       = EXCLUDED.ean_raw,
                sku_raw       = EXCLUDED.sku_raw,
                size_text     = COALESCE(EXCLUDED.size_text, public.staging_barbora_products.size_text),
                brand         = COALESCE(EXCLUDED.brand, public.staging_barbora_products.brand),
                price         = EXCLUDED.price,
                currency      = COALESCE(EXCLUDED.currency, public.staging_barbora_products.currency),
                category_path = COALESCE(EXCLUDED.category_path, public.staging_barbora_products.category_path),
                category_leaf = COALESCE(EXCLUDED.category_leaf, public.staging_barbora_products.category_leaf),
                collected_at  = now();

          INSERT INTO public.barbora_candidates
            (ext_id, ean_norm, ean_raw, sku_raw, name, size_text, brand, price, currency, category_path, category_leaf, last_seen)
          SELECT
            ext_id, ean_norm, ean_raw, sku_raw, name, size_text, brand, price, COALESCE(currency,'EUR'), category_path, category_leaf, now()
          FROM tmp_staging_barbora_products
          ON CONFLICT (ext_id) DO UPDATE
            SET price         = EXCLUDED.price,
                currency      = COALESCE(EXCLUDED.currency, public.barbora_candidates.currency),
                size_text     = CASE WHEN COALESCE(EXCLUDED.size_text,'') <> '' THEN EXCLUDED.size_text ELSE public.barbora_candidates.size_text END,
                brand         = CASE WHEN COALESCE(EXCLUDED.brand,'')     <> '' THEN EXCLUDED.brand     ELSE public.barbora_candidates.brand     END,
                category_path = CASE WHEN COALESCE(EXCLUDED.category_path,'') <> '' THEN EXCLUDED.category_path ELSE public.barbora_candidates.category_path END,
                category_leaf = CASE WHEN COALESCE(EXCLUDED.category_leaf,'') <> '' THEN EXCLUDED.category_leaf ELSE public.barbora_candidates.category_leaf END,
                ean_raw       = CASE WHEN COALESCE(EXCLUDED.ean_raw,'')   <> '' THEN EXCLUDED.ean_raw   ELSE public.barbora_candidates.ean_raw END,
                sku_raw       = CASE WHEN COALESCE(EXCLUDED.sku_raw,'')   <> '' THEN EXCLUDED.sku_raw   ELSE public.barbora_candidates.sku_raw END,
                last_seen     = now();

          COMMIT;
          SQL
          sed -i "s|__CSV__|$CSV_ABS|g" /tmp/barbora_load.sql
          psql "$DATABASE_URL" -v ON_ERROR_STOP=1 -f /tmp/barbora_load.sql

      - name: (Optional) Adopt Barbora candidates with EANs
        if: ${{ env.DATABASE_URL != '' && github.event.inputs.upsert_db == '1' }}
        shell: bash
        run: |
          set -euo pipefail
          cat > /tmp/adopt_barbora.sql <<'SQL'
          DO $do$
          DECLARE r record;
          BEGIN
            IF EXISTS (SELECT 1 FROM pg_proc WHERE proname='adopt_all_barbora_candidates_with_ean') THEN
              PERFORM public.adopt_all_barbora_candidates_with_ean();
            ELSIF EXISTS (SELECT 1 FROM pg_proc WHERE proname='adopt_candidate_with_ean') THEN
              FOR r IN
                SELECT ext_id FROM public.barbora_candidates WHERE COALESCE(ean_norm,'') <> ''
              LOOP
                PERFORM public.adopt_candidate_with_ean(r.ext_id);
              END LOOP;
            END IF;
          END
          $do$;
          SQL
          psql "$DATABASE_URL" -v ON_ERROR_STOP=1 -f /tmp/adopt_barbora.sql
