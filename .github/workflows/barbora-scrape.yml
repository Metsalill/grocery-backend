name: "Barbora (Maxima EE) – Category Crawl + optional DB upsert"

on:
  workflow_dispatch:
    inputs:
      categories_multiline:
        description: "Category URLs (one per line). If empty, auto-discover from Barbora."
        required: false
        default: ""
      page_limit:
        description: "Max pages per category (0 = all)"
        default: "0"
      max_products:
        description: "Cap total PDPs (0 = unlimited)"
        default: "0"
      headless:
        description: "Headless (1/0)"
        default: "1"
      req_delay:
        description: "Delay between steps (sec)"
        default: "0.25"
      upsert_db:
        description: "Upsert into Postgres (1=yes, 0=just CSV)"
        default: "1"
  schedule:
    - cron: "19 */3 * * *"

concurrency:
  group: barbora-category-crawl
  cancel-in-progress: true

jobs:
  crawl-and-upsert:
    runs-on: ubuntu-latest
    timeout-minutes: 110
    env:
      PYTHONUNBUFFERED: "1"
      OUTPUT_CSV: data/barbora_products.csv
      DATABASE_URL: ${{ secrets.DATABASE_URL_PUBLIC }}
      # >>> Exclusions you requested:
      EXCLUDE_SECTIONS: >-
        https://barbora.ee/kodukaubad-ja-vaba-aeg
        https://barbora.ee/enesehooldustooted
        https://barbora.ee/puhastustarbed-ja-lemmikloomatooted
        https://barbora.ee/lastekaubad

    steps:
      - uses: actions/checkout@v4

      - uses: actions/setup-python@v5
        with: { python-version: "3.11" }

      - name: Cache pip
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements-scraper.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-

      - name: Install deps (Playwright + libs)
        run: |
          set -euo pipefail
          pip install -r requirements-scraper.txt || pip install playwright beautifulsoup4 lxml pg8000 psycopg2-binary
          python -m playwright install --with-deps chromium

      - name: Verify scraper exists
        run: |
          set -euo pipefail
          [ -f scripts/barbora_crawl_categories_pw.py ] || (echo "::error::scripts/barbora_crawl_categories_pw.py missing" && exit 1)
          python -m py_compile scripts/barbora_crawl_categories_pw.py
          chmod +x scripts/barbora_crawl_categories_pw.py

      - name: Prepare workspace
        run: |
          set -euo pipefail
          rm -rf data
          mkdir -p data

      # NEW: auto-discover all category URLs from Barbora (minus excluded sections)
      - name: Auto-discover categories (unless provided via input)
        env:
          INPUT_CATS: ${{ github.event.inputs.categories_multiline }}
          EXCLUDES: ${{ env.EXCLUDE_SECTIONS }}
        run: |
          set -euo pipefail
          if [ -n "${INPUT_CATS}" ]; then
            printf "%s\n" "${INPUT_CATS}" | tr -d '\r' | sed '/^[[:space:]]*$/d' > data/barbora_categories.txt
            echo "Using categories from workflow input."
            exit 0
          fi

          python - <<'PY'
import os, re, sys, time
from urllib.parse import urljoin, urlparse
from playwright.sync_api import sync_playwright

BASE = "https://barbora.ee"
excludes = set(filter(None, os.environ.get("EXCLUDES","").split()))
discovered = set()

def absu(href: str) -> str:
    return urljoin(BASE, href)

def is_category(url: str) -> bool:
    # Barbora uses many section paths; accept “root-like” category pages
    p = urlparse(url)
    if p.netloc and p.netloc.endswith("barbora.ee"):
      # simple heuristics: single or double path segment, no query
      segs = [s for s in p.path.split("/") if s]
      if len(segs) in (1,2) and ("retseptid" not in segs) and ("info" not in segs):
          return True
    return False

with sync_playwright() as pw:
    browser = pw.chromium.launch(headless=True)
    ctx = browser.new_context()
    page = ctx.new_page()
    page.goto(BASE, timeout=45000, wait_until="domcontentloaded")

    # grab nav/menu links
    for a in page.locator("a").all():
        href = (a.get_attribute("href") or "").strip()
        if not href: continue
        u = absu(href)
        if any(u.startswith(ex) for ex in excludes): 
            continue
        if is_category(u):
            discovered.add(u)

    # also try a generic sitemap-like scan from footer and mega-menu if present
    # (already covered by 'a' grab above)

    # write result
    os.makedirs("data", exist_ok=True)
    with open("data/barbora_categories.txt","w",encoding="utf-8") as f:
        for u in sorted(discovered):
            f.write(u+"\n")

    print(f"[discover] wrote {len(discovered)} categories -> data/barbora_categories.txt")
    ctx.close()
    browser.close()
PY
          echo "TOTAL categories: $(wc -l < data/barbora_categories.txt || echo 0)"

      # --- Optional: build SKIP list so we don't re-crawl already-priced items ---
      - name: Build skip list from priced items (prices ∪ barbora_candidates)
        if: ${{ env.DATABASE_URL != '' }}
        shell: bash
        run: |
          set -euo pipefail
          : > data/barbora_skip_ext_ids.txt
          psql "$DATABASE_URL" -At -v ON_ERROR_STOP=1 <<'SQL' > data/barbora_skip_ext_ids.txt
          WITH priced_from_prices AS (
            SELECT DISTINCT (regexp_match(p.source_url, '/p/([0-9]+)'))[1] AS ext_id
            FROM public.prices p
            JOIN public.stores s ON s.id = p.store_id
            WHERE s.chain='Maxima' AND s.name ILIKE '%Barbora%' AND s.is_online = TRUE
              AND p.source_url ~ '/p/[0-9]+'
          ),
          priced_from_candidates AS (
            SELECT DISTINCT ext_id::text
            FROM public.barbora_candidates
            WHERE COALESCE(price,0) > 0
              AND COALESCE(ext_id,'') <> ''
          )
          SELECT ext_id FROM priced_from_prices
          UNION
          SELECT ext_id FROM priced_from_candidates
          ORDER BY 1;
          SQL
          echo "Skip list rows (already priced): $(wc -l < data/barbora_skip_ext_ids.txt)" | tee data/_barbora_skip_count.txt

      - name: Crawl Barbora categories (Playwright)
        env:
          REQ_DELAY: ${{ github.event.inputs.req_delay }}
          PAGE_LIMIT: ${{ github.event.inputs.page_limit }}
          MAX_PRODUCTS: ${{ github.event.inputs.max_products }}
          HEADLESS: ${{ github.event.inputs.headless }}
          SKIP_EXT_FILE: data/barbora_skip_ext_ids.txt
        run: |
          set -euo pipefail
          echo "::group::Runtime info"
          python --version
          echo "Start (UTC): $(date -u)"
          echo "::endgroup::"

          REQ_DELAY="${REQ_DELAY:-0.25}"
          PAGE_LIMIT="${PAGE_LIMIT:-0}"
          MAX_PRODUCTS="${MAX_PRODUCTS:-0}"
          HEADLESS="${HEADLESS:-1}"

          if [ -f "$SKIP_EXT_FILE" ]; then
            echo "SKIP_EXT_FILE present ($(wc -l < "$SKIP_EXT_FILE") rows)"
            SKIP_FLAG=(--skip-ext-file "$SKIP_EXT_FILE")
          else
            SKIP_FLAG=()
          fi

          set +e
          set +o pipefail
          stdbuf -oL -eL timeout -k 45s 80m \
            python -u scripts/barbora_crawl_categories_pw.py \
              --cats-file data/barbora_categories.txt \
              --page-limit "$PAGE_LIMIT" \
              --max-products "$MAX_PRODUCTS" \
              --headless "$HEADLESS" \
              --req-delay "$REQ_DELAY" \
              --output-csv "$OUTPUT_CSV" \
              "${SKIP_FLAG[@]}" \
            > >(tee data/barbora_run.log) 2>data/barbora_run.err
          status=${PIPESTATUS[0]}
          set -o pipefail
          set -e

          echo "End (UTC): $(date -u) - crawler exit code: ${status}"
          # (rest of steps unchanged: Ensure CSV, tail logs, artifacts, DB load, adopt…)
