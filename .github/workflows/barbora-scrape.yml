name: Barbora DB Scrape + Ingest

on:
  workflow_dispatch:
    inputs:
      shards:
        description: "Number of shards (lines modulo N)"
        required: false
        default: "24"
      page_limit:
        description: "Max categories to process (0 = all)"
        required: false
        default: "0"
      max_pages_per_category:
        description: "Max paginated pages per category (0 = unlimited)"
        required: false
        default: "80"
      req_delay:
        description: "Request delay seconds (float)"
        required: false
        default: "0.25"
  schedule:
    - cron: "17 2 * * *"   # run nightly; adjust as you like

jobs:
  scrape:
    name: scrape (shard ${{ matrix.shard }})
    runs-on: ubuntu-latest
    timeout-minutes: 360
    strategy:
      fail-fast: false
      matrix:
        # default to 24 shards; if user supplies a different number on dispatch,
        # we still build a 24-slot matrix and only use the first <shards> shards.
        shard: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]

    env:
      PYTHONUNBUFFERED: "1"
      PIP_DISABLE_PIP_VERSION_CHECK: "1"
      DATABASE_URL: ${{ secrets.DATABASE_URL }}   # set this in repo secrets
      STORE_ID: "441"                              # Barbora ePood (override if needed)
      # Soft time-budget so the crawler flushes CSV + ingests BEFORE GH job cap
      SOFT_TIMEOUT_MIN: "320"                      # ~5h20m

    steps:
      - name: Checkout repo
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Cache pip
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}

      - name: Install crawler deps (pip)
        run: |
          python -m pip install -U pip
          pip install -r requirements.txt

      - name: Cache Playwright browsers
        uses: actions/cache@v4
        with:
          path: ~/.cache/ms-playwright
          key: ${{ runner.os }}-ms-playwright-${{ hashFiles('**/requirements.txt') }}

      - name: Install Playwright (Chromium)
        run: |
          python -m playwright install --with-deps chromium

      - name: Prepare shard file
        id: prep
        shell: bash
        env:
          SHARDS: ${{ inputs.shards != '' && inputs.shards || '24' }}
        run: |
          set -euo pipefail
          CATS="data/barbora_categories.txt"
          if [[ ! -f "$CATS" ]]; then
            echo "Missing $CATS" >&2
            exit 1
          fi
          mkdir -p "$RUNNER_TEMP/cats"
          # Build shard file for THIS matrix index using modulo on line number
          awk -v n="${SHARDS}" -v s="${{ matrix.shard }}" 'NR>0 { if ((NR-1) % n == s) print }' "$CATS" > "$RUNNER_TEMP/cats/cats_${{ matrix.shard }}.txt"
          echo "cats_file=$RUNNER_TEMP/cats/cats_${{ matrix.shard }}.txt" >> "$GITHUB_OUTPUT"

      - name: Run Barbora crawler
        shell: bash
        env:
          REQ_DELAY: ${{ inputs.req_delay != '' && inputs.req_delay || '0.25' }}
          PAGE_LIMIT: ${{ inputs.page_limit != '' && inputs.page_limit || '0' }}
          MAX_PAGES_PER_CATEGORY: ${{ inputs.max_pages_per_category != '' && inputs.max_pages_per_category || '80' }}
        run: |
          set -euo pipefail
          mkdir -p out
          OUT="out/barbora_shard_${{ matrix.shard }}.csv"
          python scripts/barbora_crawl_categories_pw.py \
            --cats-file "${{ steps.prep.outputs.cats_file }}" \
            --page-limit "${PAGE_LIMIT}" \
            --max-pages-per-category "${MAX_PAGES_PER_CATEGORY}" \
            --headless true \
            --req-delay "${REQ_DELAY}" \
            --out-csv "${OUT}"
          echo "Wrote ${OUT}"

      - name: Upload CSV artifact (per shard)
        uses: actions/upload-artifact@v4
        with:
          name: barbora_shard_${{ matrix.shard }}     # no slashes â†’ artifact name is valid
          path: out/barbora_shard_${{ matrix.shard }}.csv
          if-no-files-found: warn
