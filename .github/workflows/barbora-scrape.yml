name: "Barbora (Maxima EE) - Category Crawl + optional DB upsert"

on:
  workflow_dispatch:
    inputs:
      mode:
        description: "prices (default) or metadata (crawl only items missing brand/EAN/manufacturer)"
        required: false
        default: "prices"
      categories_multiline:
        description: "Category URLs (one per line). If empty, auto-discover from Barbora."
        required: false
        default: ""
      page_limit:
        description: "Max pages per category (0 = all)"
        required: false
        default: "0"
      max_products:
        description: "Cap total PDPs (0 = unlimited)"
        required: false
        default: "0"
      headless:
        description: "Headless (1/0)"
        required: false
        default: "1"
      req_delay:
        description: "Delay between steps (sec)"
        required: false
        default: "0.25"
      upsert_db:
        description: "Upsert into Postgres (1=yes, 0=just CSV)"
        required: false
        default: "1"
  schedule:
    - cron: "19 3 * * 0"   # Sun at 03:19 UTC

concurrency:
  group: barbora-category-crawl
  cancel-in-progress: true

jobs:
  crawl-and-upsert:
    name: crawl-and-upsert (shard ${{ matrix.shard }})
    runs-on: ubuntu-latest
    timeout-minutes: 110

    strategy:
      fail-fast: false
      matrix:
        shard: [0, 1]

    env:
      PYTHONUNBUFFERED: "1"
      OUTPUT_CSV: data/barbora_products.csv
      DATABASE_URL: ${{ secrets.DATABASE_URL_PUBLIC }}
      MODE: ${{ github.event.inputs.mode || 'prices' }}
      # Space-separated URL prefixes to exclude/include
      EXCLUDE_SECTIONS: "https://barbora.ee/kodukaubad-ja-vaba-aeg https://barbora.ee/enesehooldustooted https://barbora.ee/puhastustarbed-ja-lemmikloomatooted https://barbora.ee/lastekaubad"
      INCLUDE_SECTIONS: "https://barbora.ee/enesehooldustooted/suuhugieen/hambapastad https://barbora.ee/enesehooldustooted/raseerimisvahendid https://barbora.ee/enesehooldustooted/intiimhugieeni-vahendid https://barbora.ee/puhastustarbed-ja-lemmikloomatooted/pesupesemisvahendid https://barbora.ee/puhastustarbed-ja-lemmikloomatooted/noudepesuvahendid https://barbora.ee/puhastustarbed-ja-lemmikloomatooted/kodukeemia https://barbora.ee/puhastustarbed-ja-lemmikloomatooted/majapidamis-ja-koristustarbed https://barbora.ee/lastekaubad/piimasegud-ja-jatkupiimasegud https://barbora.ee/lastekaubad/pudrud https://barbora.ee/lastekaubad/mahkmed https://barbora.ee/lastekaubad/puuviljapureed https://barbora.ee/lastekaubad/laste-hugieenitarbed/niisked-salvratikud https://barbora.ee/lastekaubad/liha-ja-koogiviljapureed"
      # Ensure scheduled & manual runs upsert by default (unless user overrides)
      UPSERT_DB: ${{ github.event_name == 'workflow_dispatch' && github.event.inputs.upsert_db || '1' }}
      # Shard index from matrix
      SHARD: ${{ matrix.shard }}

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install deps (Playwright + libs)
        shell: bash
        run: |
          set -euo pipefail
          pip install playwright beautifulsoup4 lxml pg8000 psycopg2-binary
          python -m playwright install --with-deps chromium

      - name: Verify scraper exists
        shell: bash
        run: |
          set -euo pipefail
          if [ ! -f scripts/barbora_crawl_categories_pw.py ]; then
            echo "::error::scripts/barbora_crawl_categories_pw.py missing"
            exit 1
          fi
          python -m py_compile scripts/barbora_crawl_categories_pw.py
          chmod +x scripts/barbora_crawl_categories_pw.py

      - name: Prepare workspace
        shell: bash
        run: |
          set -euo pipefail
          rm -rf data
          mkdir -p data

      - name: Auto-discover categories (unless provided via input)
        env:
          INPUT_CATS: ${{ github.event.inputs.categories_multiline }}
          EXCLUDES: ${{ env.EXCLUDE_SECTIONS }}
          INCLUDES: ${{ env.INCLUDE_SECTIONS }}
        shell: bash
        run: |
          set -euo pipefail
          mkdir -p data

          # 1) Explicit input wins
          if [ -n "${INPUT_CATS}" ]; then
            printf "%s\n" "${INPUT_CATS}" | tr -d '\r' | sed '/^[[:space:]]*$/d' > data/barbora_categories.txt
            echo "Using categories from workflow input."
          # 2) Repo file next (if already committed)
          elif [ -s data/barbora_categories.txt ]; then
            echo "Using repo file data/barbora_categories.txt"
          else
            # 3) Try autodiscover from homepage
            {
              echo 'import os'
              echo 'from urllib.parse import urljoin, urlparse'
              echo 'from playwright.sync_api import sync_playwright'
              echo 'BASE = "https://barbora.ee"'
              echo 'excludes = tuple(os.environ.get("EXCLUDES","").split())'
              echo 'includes = tuple(os.environ.get("INCLUDES","").split())'
              echo 'discovered = set()'
              echo 'def absu(href): return urljoin(BASE, href)'
              echo 'def is_category(url):'
              echo '    p = urlparse(url)'
              echo '    if not (p.scheme and p.netloc and p.netloc.endswith("barbora.ee")): return False'
              echo '    segs = [s for s in p.path.split("/") if s]'
              echo '    # allow 1..4 segments; avoid obvious non-catalog roots'
              echo '    if any(s in ("retseptid","info","blogi","kampaania") for s in segs): return False'
              echo '    return 1 <= len(segs) <= 4'
              echo 'def allowed(url):'
              echo '    if any(url.startswith(pref) for pref in includes): return True'
              echo '    if any(url.startswith(pref) for pref in excludes): return False'
              echo '    return is_category(url)'
              echo 'with sync_playwright() as pw:'
              echo '    b = pw.chromium.launch(headless=True)'
              echo '    ctx = b.new_context()'
              echo '    p = ctx.new_page()'
              echo '    p.goto(BASE, timeout=45000, wait_until="domcontentloaded")'
              echo '    for a in p.locator("a").all():'
              echo '        href = (a.get_attribute("href") or "").strip()'
              echo '        if not href: continue'
              echo '        u = absu(href)'
              echo '        if allowed(u): discovered.add(u)'
              echo '    os.makedirs("data", exist_ok=True)'
              echo '    with open("data/barbora_categories.txt","w",encoding="utf-8") as f:'
              echo '        for u in sorted(discovered): f.write(u + "\n")'
              echo '    print(f"[discover] wrote {len(discovered)} -> data/barbora_categories.txt")'
              echo '    ctx.close(); b.close()'
            } > /tmp/discover_barbora.py
            python /tmp/discover_barbora.py

            # 4) Fallback to a curated FOOD roots list if autodiscover empty/too small
            if [ ! -s data/barbora_categories.txt ] || [ "$(wc -l < data/barbora_categories.txt)" -lt 10 ]; then
              printf '%s\n' \
                'https://barbora.ee/koogiviljad-puuviljad' \
                'https://barbora.ee/piimatooted-munad-void' \
                'https://barbora.ee/juustud' \
                'https://barbora.ee/leib-sai-kondiitritooted' \
                'https://barbora.ee/valmistoidud' \
                'https://barbora.ee/kuivained-ja-hoidised' \
                'https://barbora.ee/maitseained-ja-kastmed' \
                'https://barbora.ee/suupisted-maiustused' \
                'https://barbora.ee/joogid' \
                'https://barbora.ee/kulmutatud-tooted' \
                > data/barbora_categories.txt
              echo "Autodiscover small → wrote curated FOOD roots to data/barbora_categories.txt"
            fi
          fi

          echo "TOTAL categories: $(wc -l < data/barbora_categories.txt || echo 0)"
          echo "--- preview ---"
          head -n 30 data/barbora_categories.txt || true
          echo "--------------"
          test -s data/barbora_categories.txt

      - name: Shard category list (2-way)
        shell: bash
        env:
          SHARDS: 2
        run: |
          set -euo pipefail
          cp -f data/barbora_categories.txt data/barbora_categories_all.txt
          TOTAL=$(wc -l < data/barbora_categories_all.txt)
          : "${SHARD:?SHARD env is required}"
          awk -v s="$SHARD" -v n="$SHARDS" 'NR>0 { if ((NR-1)%n==s) print }' \
            data/barbora_categories_all.txt > data/barbora_categories.txt
          echo "TOTAL categories: $TOTAL"
          echo "Shard: $SHARD / $SHARDS"
          echo "Slice count: $(wc -l < data/barbora_categories.txt)"
          head -n 12 data/barbora_categories.txt || true

      # Ensure psql available for the skip/only steps
      - name: Install psql client (for skip/only lists)
        if: ${{ env.DATABASE_URL != '' }}
        shell: bash
        run: |
          sudo apt-get update
          sudo apt-get install -y postgresql-client

      # MODE=prices → skip only items that are priced AND already have a brand + name
      - name: Build skip list (priced & have brand+name)
        if: ${{ env.DATABASE_URL != '' && env.MODE == 'prices' }}
        shell: bash
        run: |
          set -euo pipefail
          : > data/barbora_skip_ext_ids.txt

          psql "$DATABASE_URL" -At -v ON_ERROR_STOP=1 <<'SQL' > data/barbora_skip_ext_ids.txt
          WITH priced_from_prices AS (
            SELECT DISTINCT p.source_url AS ext_id
            FROM public.prices p
            JOIN public.stores s ON s.id = p.store_id
            WHERE s.chain='Maxima' AND s.name ILIKE '%Barbora%' AND s.is_online = TRUE
              AND p.source_url ILIKE 'https://barbora.ee%'
          ),
          priced_from_candidates AS (
            SELECT DISTINCT ext_id::text
            FROM public.barbora_candidates
            WHERE COALESCE(price,0) > 0 AND COALESCE(ext_id,'') <> ''
          ),
          priced AS (
            SELECT ext_id FROM priced_from_prices
            UNION
            SELECT ext_id FROM priced_from_candidates
          )
          SELECT pr.ext_id
          FROM priced pr
          LEFT JOIN public.barbora_candidates bc ON bc.ext_id = pr.ext_id
          LEFT JOIN public.staging_barbora_products sp ON sp.ext_id = pr.ext_id
          WHERE COALESCE(NULLIF(btrim(COALESCE(bc.brand, sp.brand, '')), ''), NULL) IS NOT NULL
            AND COALESCE(NULLIF(btrim(COALESCE(bc.name, sp.name, '')), ''), NULL) IS NOT NULL
          ORDER BY 1;
          SQL

          echo "Skip list rows (priced & have brand+name): $(wc -l < data/barbora_skip_ext_ids.txt)" | tee data/_barbora_skip_count.txt

      # MODE=metadata → crawl only items missing brand/EAN/manufacturer (fallback-safe)
      - name: Build ONLY list (missing brand/EAN/manufacturer; fallback-safe)
        if: ${{ env.DATABASE_URL != '' && env.MODE == 'metadata' }}
        shell: bash
        run: |
          set -euo pipefail
          : > data/barbora_only_ext_ids.txt

          HAS_MANU="$(psql "$DATABASE_URL" -Atc "
            WITH cols AS (
              SELECT table_name
              FROM information_schema.columns
              WHERE table_schema='public'
                AND column_name='manufacturer'
                AND table_name IN ('barbora_candidates','staging_barbora_products')
            )
            SELECT CASE WHEN COUNT(*)=2 THEN '1' ELSE '0' END FROM cols;
          ")"
          echo "HAS_MANUFACTURER_COLS=${HAS_MANU}"

          if [ "${HAS_MANU}" = "1" ]; then
            psql "$DATABASE_URL" -At -v ON_ERROR_STOP=1 -c "
              WITH all_ids AS (
                SELECT ext_id FROM public.barbora_candidates
                UNION
                SELECT ext_id FROM public.staging_barbora_products
              )
              SELECT a.ext_id
              FROM all_ids a
              LEFT JOIN public.barbora_candidates bc ON bc.ext_id = a.ext_id
              LEFT JOIN public.staging_barbora_products sp ON sp.ext_id = a.ext_id
              WHERE
                COALESCE(NULLIF(btrim(COALESCE(bc.brand, sp.brand, '')), ''), NULL) IS NULL
                OR COALESCE(NULLIF(regexp_replace(COALESCE(bc.ean_raw, sp.ean_raw, ''), '[^0-9]', '', 'g'), ''), NULL) IS NULL
                OR COALESCE(NULLIF(btrim(COALESCE(bc.manufacturer, sp.manufacturer, '')), ''), NULL) IS NULL
              ORDER BY 1;
            " > data/barbora_only_ext_ids.txt
          else
            echo "::warning::manufacturer column missing -> falling back to (brand/EAN/name)"
            psql "$DATABASE_URL" -At -v ON_ERROR_STOP=1 -c "
              WITH all_ids AS (
                SELECT ext_id FROM public.barbora_candidates
                UNION
                SELECT ext_id FROM public.staging_barbora_products
              )
              SELECT a.ext_id
              FROM all_ids a
              LEFT JOIN public.barbora_candidates bc ON bc.ext_id = a.ext_id
              LEFT JOIN public.staging_barbora_products sp ON sp.ext_id = a.ext_id
              WHERE
                COALESCE(NULLIF(btrim(COALESCE(bc.brand, sp.brand, '')), ''), NULL) IS NULL
                OR COALESCE(NULLIF(regexp_replace(COALESCE(bc.ean_raw, sp.ean_raw, ''), '[^0-9]', '', 'g'), ''), NULL) IS NULL
                OR COALESCE(NULLIF(btrim(COALESCE(bc.name,  sp.name,  '')), ''), NULL) IS NULL
              ORDER BY 1;
            " > data/barbora_only_ext_ids.txt
          fi

          echo "ONLY list rows: $(wc -l < data/barbora_only_ext_ids.txt)"

      - name: Crawl Barbora categories (Playwright)
        env:
          REQ_DELAY: ${{ github.event.inputs.req_delay }}
          PAGE_LIMIT: ${{ github.event.inputs.page_limit }}
          MAX_PRODUCTS: ${{ github.event.inputs.max_products }}
          HEADLESS: ${{ github.event.inputs.headless }}
        shell: bash
        run: |
          set -euo pipefail
          REQ_DELAY="${REQ_DELAY:-0.25}"
          PAGE_LIMIT="${PAGE_LIMIT:-0}"
          MAX_PRODUCTS="${MAX_PRODUCTS:-0}"
          HEADLESS="${HEADLESS:-1}"

          SKIP_FLAG=()
          ONLY_FLAG=()
          if [ "${MODE}" = "prices" ] && [ -s data/barbora_skip_ext_ids.txt ]; then
            SKIP_FLAG=( --skip-ext-file data/barbora_skip_ext_ids.txt )
          fi
          if [ "${MODE}" = "metadata" ] && [ -s data/barbora_only_ext_ids.txt ]; then
            ONLY_FLAG=( --only-ext-file data/barbora_only_ext_ids.txt )
          fi

          cmd=( python -u scripts/barbora_crawl_categories_pw.py
                --cats-file data/barbora_categories.txt
                --page-limit "$PAGE_LIMIT"
                --max-products "$MAX_PRODUCTS"
                --headless "$HEADLESS"
                --req-delay "$REQ_DELAY"
                --output-csv "$OUTPUT_CSV"
                "${SKIP_FLAG[@]}"
                "${ONLY_FLAG[@]}" )

          # Increased timeout to use the job window better
          stdbuf -oL -eL timeout -k 60s 107m "${cmd[@]}" | tee data/barbora_run.log

      - name: Ensure CSV exists (header if empty)
        shell: bash
        run: |
          set -euo pipefail
          if [ ! -s "$OUTPUT_CSV" ]; then
            mkdir -p "$(dirname "$OUTPUT_CSV")"
            echo 'store_chain,store_name,store_channel,ext_id,ean_raw,sku_raw,name,size_text,brand,manufacturer,price,currency,image_url,category_path,category_leaf,source_url' > "$OUTPUT_CSV"
            echo "Wrote header-only CSV -> $OUTPUT_CSV"
          fi

      - name: Upload crawl artifacts
        if: ${{ always() }}
        uses: actions/upload-artifact@v4
        with:
          name: barbora-crawl-${{ env.MODE }}-shard-${{ matrix.shard }}-${{ github.run_id }}
          path: |
            data/barbora_products.csv
            data/barbora_run.log
            data/_barbora_skip_count.txt
            data/barbora_skip_ext_ids.txt
            data/barbora_only_ext_ids.txt
          if-no-files-found: warn
          retention-days: 7

      # ---------- Optional DB part ----------
      - name: Install psql client
        if: ${{ env.DATABASE_URL != '' && env.UPSERT_DB == '1' }}
        shell: bash
        run: |
          sudo apt-get update
          sudo apt-get install -y postgresql-client

      - name: DB sanity (connection)
        if: ${{ env.DATABASE_URL != '' && env.UPSERT_DB == '1' }}
        shell: bash
        run: |
          set -euo pipefail
          psql "$DATABASE_URL" -c "\conninfo" || true
          psql "$DATABASE_URL" -c "SELECT current_database(), current_user;" || true

      - name: Prepare DB (store + staging)
        if: ${{ env.DATABASE_URL != '' && env.UPSERT_DB == '1' }}
        shell: bash
        run: |
          psql "$DATABASE_URL" -v ON_ERROR_STOP=1 <<'SQL'
          BEGIN;
          INSERT INTO public.stores (name, chain, is_online)
          SELECT 'Barbora ePood', 'Maxima', TRUE
          WHERE NOT EXISTS (SELECT 1 FROM public.stores WHERE name='Barbora ePood' AND chain='Maxima' AND is_online=TRUE);

          CREATE TABLE IF NOT EXISTS public.staging_barbora_products (
            ext_id        text PRIMARY KEY,
            name          text NOT NULL,
            ean_raw       text,
            sku_raw       text,
            ean_norm      text GENERATED ALWAYS AS (regexp_replace(COALESCE(ean_raw,''), '[^0-9]', '', 'g')) STORED,
            size_text     text,
            brand         text,
            manufacturer  text,
            price         numeric(12,2),
            currency      text DEFAULT 'EUR',
            category_path text,
            category_leaf text,
            source_url    text,
            collected_at  timestamptz DEFAULT now()
          );
          CREATE EXTENSION IF NOT EXISTS pg_trgm;
          CREATE INDEX IF NOT EXISTS ix_barbora_ean       ON public.staging_barbora_products (ean_norm);
          CREATE INDEX IF NOT EXISTS ix_barbora_name_trgm ON public.staging_barbora_products USING gin (name gin_trgm_ops);

          CREATE TABLE IF NOT EXISTS public.barbora_candidates (
            ext_id        text PRIMARY KEY,
            ean_norm      text,
            ean_raw       text,
            sku_raw       text,
            name          text,
            size_text     text,
            brand         text,
            manufacturer  text,
            price         numeric(12,2),
            currency      text,
            category_path text,
            category_leaf text,
            source_url    text,
            last_seen     timestamptz DEFAULT now()
          );
          COMMIT;
          SQL

      - name: Load CSV to DB
        if: ${{ env.DATABASE_URL != '' && env.UPSERT_DB == '1' }}
        shell: bash
        run: |
          set -euo pipefail
          CSV_ABS="$(python -c 'import os;print(os.path.abspath("data/barbora_products.csv"))')"
          cat > /tmp/barbora_load.sql <<'SQL'
          \set ON_ERROR_STOP on
          BEGIN;

          CREATE TEMP TABLE tmp_barbora_csv_full (
            store_chain   text,
            store_name    text,
            store_channel text,
            ext_id        text,
            ean_raw       text,
            sku_raw       text,
            name          text,
            size_text     text,
            brand         text,
            manufacturer  text,
            price         text,
            currency      text,
            image_url     text,
            category_path text,
            category_leaf text,
            source_url    text
          );

          \copy tmp_barbora_csv_full FROM '__CSV__' CSV HEADER

          -- Normalize and fill blanks; build a clean staging temp table.
          CREATE TEMP TABLE tmp_staging_barbora_products AS
          SELECT
            ext_id,
            -- Fill missing/blank name from slug
            COALESCE(NULLIF(btrim(name), ''), initcap(replace(regexp_replace(COALESCE(ext_id,''), '^.*/', ''), '-', ' '))) AS name,
            NULLIF(btrim(ean_raw), '') AS ean_raw,
            NULLIF(btrim(sku_raw), '') AS sku_raw,
            NULLIF(regexp_replace(COALESCE(ean_raw,''), '[^0-9]', '', 'g'), '') AS ean_norm,
            NULLIF(btrim(size_text), '') AS size_text,
            NULLIF(btrim(brand), '') AS brand,
            NULLIF(btrim(manufacturer), '') AS manufacturer,
            -- normalize price "3,99" → 3.99
            NULLIF(regexp_replace(price, ',', '.', 'g'), '')::numeric AS price,
            UPPER(COALESCE(NULLIF(currency,''), 'EUR')) AS currency,
            NULLIF(btrim(category_path), '') AS category_path,
            NULLIF(btrim(category_leaf), '') AS category_leaf,
            NULLIF(btrim(source_url), '') AS source_url
          FROM tmp_barbora_csv_full
          WHERE COALESCE(ext_id,'') <> '';

          -- Drop any rows that still lack a name after fallback
          DELETE FROM tmp_staging_barbora_products
          WHERE COALESCE(name,'') = '';

          INSERT INTO public.staging_barbora_products
            (ext_id,name,ean_raw,sku_raw,size_text,brand,manufacturer,price,currency,category_path,category_leaf,source_url,collected_at)
          SELECT ext_id,name,ean_raw,sku_raw,size_text,brand,manufacturer,price,currency,category_path,category_leaf,source_url,now()
          FROM tmp_staging_barbora_products
          ON CONFLICT (ext_id) DO UPDATE
            SET name          = EXCLUDED.name,
                ean_raw       = EXCLUDED.ean_raw,
                sku_raw       = EXCLUDED.sku_raw,
                size_text     = COALESCE(EXCLUDED.size_text, public.staging_barbora_products.size_text),
                brand         = COALESCE(EXCLUDED.brand, public.staging_barbora_products.brand),
                manufacturer  = COALESCE(EXCLUDED.manufacturer, public.staging_barbora_products.manufacturer),
                price         = EXCLUDED.price,
                currency      = COALESCE(EXCLUDED.currency, public.staging_barbora_products.currency),
                category_path = COALESCE(EXCLUDED.category_path, public.staging_barbora_products.category_path),
                category_leaf = COALESCE(EXCLUDED.category_leaf, public.staging_barbora_products.category_leaf),
                source_url    = COALESCE(EXCLUDED.source_url, public.staging_barbora_products.source_url),
                collected_at  = now();

          INSERT INTO public.barbora_candidates
            (ext_id, ean_norm, ean_raw, sku_raw, name, size_text, brand, manufacturer, price, currency, category_path, category_leaf, source_url, last_seen)
          SELECT
            ext_id, ean_norm, ean_raw, sku_raw, name, size_text, brand, manufacturer, price, currency, category_path, category_leaf, source_url, now()
          FROM tmp_staging_barbora_products
          ON CONFLICT (ext_id) DO UPDATE
            SET price         = EXCLUDED.price,
                currency      = COALESCE(EXCLUDED.currency, public.barbora_candidates.currency),
                size_text     = CASE WHEN COALESCE(EXCLUDED.size_text,'')     <> '' THEN EXCLUDED.size_text     ELSE public.barbora_candidates.size_text END,
                brand         = CASE WHEN COALESCE(EXCLUDED.brand,'')         <> '' THEN EXCLUDED.brand         ELSE public.barbora_candidates.brand     END,
                manufacturer  = CASE WHEN COALESCE(EXCLUDED.manufacturer,'')  <> '' THEN EXCLUDED.manufacturer  ELSE public.barbora_candidates.manufacturer END,
                category_path = CASE WHEN COALESCE(EXCLUDED.category_path,'') <> '' THEN EXCLUDED.category_path ELSE public.barbora_candidates.category_path END,
                category_leaf = CASE WHEN COALESCE(EXCLUDED.category_leaf,'') <> '' THEN EXCLUDED.category_leaf ELSE public.barbora_candidates.category_leaf END,
                ean_raw       = CASE WHEN COALESCE(EXCLUDED.ean_raw,'')       <> '' THEN EXCLUDED.ean_raw       ELSE public.barbora_candidates.ean_raw END,
                sku_raw       = CASE WHEN COALESCE(EXCLUDED.sku_raw,'')       <> '' THEN EXCLUDED.sku_raw       ELSE public.barbora_candidates.sku_raw END,
                source_url    = COALESCE(EXCLUDED.source_url, public.barbora_candidates.source_url),
                last_seen     = now();

          COMMIT;

          -- Quick feedback
          \echo ''
          \echo '=== Load summary ==='
          SELECT
            COUNT(*) AS candidates_total,
            SUM((price IS NULL OR price = 0)::int) AS zero_price
          FROM public.barbora_candidates;

          SQL
          sed -i "s|__CSV__|$CSV_ABS|g" /tmp/barbora_load.sql
          psql "$DATABASE_URL" -v ON_ERROR_STOP=1 -f /tmp/barbora_load.sql
