name: Barbora DB Scrape + Ingest

on:
  schedule:
    - cron: "17 2 * * *"      # daily at ~02:17
  workflow_dispatch:

env:
  SHARDS: "24"                # increase if still slow (try 32)
  PAGE_LIMIT: "60"            # safety cap per category
  MAX_PAGES_PER_CATEGORY: "18"
  PDP_WORKERS: "2"            # keep small to avoid rate limits
  REQ_DELAY: "0.35"           # seconds between requests
  NAV_TIMEOUT: "420"          # seconds (7 minutes)
  DB_POOL_SIZE: "1"           # keep DB pool tiny per job
  PYTHONUTF8: "1"

concurrency:
  group: barbora-scrape
  cancel-in-progress: false

jobs:
  scrape:
    name: scrape (shard ${{ matrix.shard }})
    timeout-minutes: 150
    runs-on: ubuntu-latest
    strategy:
      fail-fast: false
      matrix:
        shard: [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23]

    steps:
      - name: Checkout repo
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"
          cache: "pip"
          cache-dependency-path: |
            requirements.txt
            requirements-crawler.txt

      - name: Install crawler deps (pip)
        run: |
          python -m pip install -U pip
          pip install -r requirements-crawler.txt

      - name: Install Playwright (browser cache)
        run: |
          python -m playwright install --with-deps chromium

      - name: Prepare shard files
        shell: python
        run: |
          from pathlib import Path
          cats = [l.strip() for l in Path("data/barbora_categories.txt").read_text().splitlines() if l.strip()]
          shards = int("${{ env.SHARDS }}")
          outdir = Path("data/shards"); outdir.mkdir(parents=True, exist_ok=True)
          # round-robin split to balance long/short categories
          parts = [[] for _ in range(shards)]
          for i, c in enumerate(cats):
            parts[i % shards].append(c)
          for i, p in enumerate(parts):
            Path(outdir / f"barbora_shard_{i}.txt").write_text("\n".join(p))

      - name: Run Barbora crawler
        env:
          # throttle DB connections per shard
          PGPOOL_MAX_SIZE: ${{ env.DB_POOL_SIZE }}
        run: |
          mkdir -p out
          python scripts/barbora_crawl_categories_pw.py \
            --cats-file "data/shards/barbora_shard_${{ matrix.shard }}.txt" \
            --output-csv "out/barbora_${{ matrix.shard }}.csv" \
            --headless \
            --page-limit ${{ env.PAGE_LIMIT }} \
            --max-pages-per-category ${{ env.MAX_PAGES_PER_CATEGORY }} \
            --pdp-workers ${{ env.PDP_WORKERS }} \
            --req-delay ${{ env.REQ_DELAY }} \
            --nav-timeout ${{ env.NAV_TIMEOUT }} \
            --db-pool-size ${{ env.DB_POOL_SIZE }} \
            --upsert-db main

      - name: Upload CSV artifact (per shard)
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: barbora_${{ matrix.shard }}.csv
          path: out/barbora_${{ matrix.shard }}.csv
          if-no-files-found: ignore
