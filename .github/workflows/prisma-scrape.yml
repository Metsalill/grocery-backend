name: Prisma DB Scrape + CSV Artifact

on:
  workflow_dispatch: {}
  schedule:
    # Every 5 hours (UTC)
    - cron: "0 */24 * * *"

concurrency:
  group: prisma-db-scrape
  cancel-in-progress: true

jobs:
  scrape:
    runs-on: ubuntu-latest
    # Keep well below 4h; the chunk loop below totals ~150m of work
    timeout-minutes: 200
    env:
      DATABASE_URL: ${{ secrets.RW_DATABASE_URL }}   # Railway URL secret
      PRODUCTS_TABLE: products
      PYTHONUNBUFFERED: "1"

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"
          cache: "pip"

      - name: Install deps
        run: |
          set -euo pipefail
          python -V
          pip install --upgrade pip
          pip install playwright psycopg2-binary
          # If your repo has a requirements.txt, try it first (non-fatal if missing)
          pip install -r requirements.txt || true

      - name: Install Chromium + deps
        run: python -m playwright install --with-deps chromium

      # --- Diagnostics: verify the secret and DB connectivity ---
      - name: Print DATABASE_URL host/db
        run: >
          python -c
          "import os,urllib.parse as u; x=u.urlparse(os.environ['DATABASE_URL']);
          print('DATABASE_URL host:', x.hostname, 'db:', x.path.lstrip('/'))"

      - name: Ensure not Supabase
        run: >
          python -c
          "import os,sys,urllib.parse as u; h=u.urlparse(os.environ['DATABASE_URL']).hostname;
          (sys.exit(2) if (h and 'supabase.co' in h) else print('OK: Railway host detected'))"

      - name: Connect test (DB identity)
        run: >
          python -c
          "import os,psycopg2 as p;
          c=p.connect(os.environ['DATABASE_URL']);cur=c.cursor();
          cur.execute('SELECT current_database(), inet_server_addr();');
          print('DB identity:', cur.fetchone()); c.close()"
      # ------------------------------------------------------------------------

      # Run the big scrape as bounded chunks so we always finish the workflow.
      # Each chunk is limited by a wall-clock timeout; non-zero exits won't fail the job.
      # PRELOAD_DB* tells the scraper to fetch already-seen Prisma URLs and skip them.
      - name: Prisma → DB (UPSERT by EAN) in chunks
        shell: bash
        env:
          CHUNKS: "6"             # number of passes
          PER_CHUNK_LIMIT: "2200" # products per pass (script enforces --max-products)
          CHUNK_TIMEOUT: "25m"    # wall-clock per pass (keeps total under ~150m)
          PRELOAD_DB: "1"
          PRELOAD_DB_QUERY: |
            SELECT DISTINCT p.source_url
            FROM public.prices p
            JOIN public.stores s ON s.id = p.store_id
            WHERE s.chain = 'Prisma'
              AND s.is_online = TRUE
              AND p.source_url IS NOT NULL
              AND p.source_url <> ''
              AND p.collected_at > now() - interval '60 days';
          PRELOAD_DB_LIMIT: "0"   # 0 = no cap
          PGSSLMODE: "require"
        run: |
          set -euo pipefail
          echo "Running $CHUNKS chunks; limit=$PER_CHUNK_LIMIT; timeout=$CHUNK_TIMEOUT per chunk"
          for i in $(seq 1 "$CHUNKS"); do
            echo "::group::Chunk $i/$CHUNKS"
            set +e
            timeout -k 30s "$CHUNK_TIMEOUT" \
              python scripts/prisma_food_scrape_to_db.py \
                --max-products "$PER_CHUNK_LIMIT" \
                --headless 1
            status=$?
            set -e
            if [ "$status" -eq 124 ]; then
              echo "::warning::Chunk $i timed out (expected). Continuing."
            elif [ "$status" -ne 0 ]; then
              echo "::warning::Chunk $i failed with exit $status; continuing."
            else
              echo "Chunk $i completed."
            fi
            echo "::endgroup::"
            # small pause between chunks to avoid bursty load on target
            sleep 5
          done

      - name: Produce CSV export (DB → CSV)
        run: |
          set -euo pipefail
          python scripts/db_export_csv.py prisma_foods.csv
          echo "CSV line count (incl header):"; wc -l prisma_foods.csv || true
          echo "Preview:"; head -n 5 prisma_foods.csv || true

      - name: Upload CSV artifact
        uses: actions/upload-artifact@v4
        with:
          name: prisma_foods.csv
          path: prisma_foods.csv
          if-no-files-found: error
