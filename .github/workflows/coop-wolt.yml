name: "Coop Wolt – Crawl (Pärnu default, 2-hourly)"

on:
  schedule:
    - cron: "0 */2 * * *"   # every 2 hours, at minute 0
  workflow_dispatch:
    inputs:
      store:
        description: "Which Wolt Coop store to crawl"
        required: false
        default: "parnu"
        type: choice
        options: [parnu, lasname, all]
      max_products:
        description: "Cap per category after discovery (0 = unlimited)"
        required: false
        default: "0"
      upsert_db:
        description: "1 = upsert into DATABASE_URL_PUBLIC (staging_coop_products)"
        required: false
        default: "0"
      force_playwright:
        description: "Force Playwright fallback (debug)"
        required: false
        default: "0"

jobs:
  crawl:
    runs-on: ubuntu-latest
    timeout-minutes: 180
    outputs:
      stores: ${{ steps.decide.outputs.stores }}

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      # Decide which store(s) to crawl -> JSON array in an output we can fromJSON()
      - name: Decide store matrix
        id: decide
        run: |
          if [ "${{ github.event_name }}" != "workflow_dispatch" ]; then
            # scheduled run → default to Pärnu only
            echo '["parnu"]' > /tmp/stores.json
          else
            case "${{ inputs.store }}" in
              all)     echo '["lasname","parnu"]' > /tmp/stores.json ;;
              parnu)   echo '["parnu"]'          > /tmp/stores.json ;;
              lasname) echo '["lasname"]'        > /tmp/stores.json ;;
              *)       echo '["parnu"]'          > /tmp/stores.json ;;
            esac
          fi
          echo "stores=$(cat /tmp/stores.json)" >> "$GITHUB_OUTPUT"

  # We need strategy after we have crawl.outputs.stores
  run-crawlers:
    runs-on: ubuntu-latest
    timeout-minutes: 180
    needs: crawl
    concurrency:
      group: coop-wolt-${{ matrix.store }}
      cancel-in-progress: true

    strategy:
      fail-fast: false
      max-parallel: 1
      matrix:
        store: ${{ fromJSON(needs.crawl.outputs.stores || '["parnu"]') }}

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install deps (pip + Playwright)
        run: |
          python -m pip install --upgrade pip
          if [ -f requirements.txt ]; then pip install -r requirements.txt; fi
          pip install playwright asyncpg
          python -m playwright install --with-deps chromium

      - name: Prepare categories file
        id: cats
        run: |
          set -euo pipefail
          mkdir -p out
          case "${{ matrix.store }}" in
            parnu)   SRC="data/wolt_coop_parnu_categories.txt" ;;
            lasname) SRC="data/wolt_coop_lasname_categories.txt" ;;
            *)       SRC="data/wolt_coop_parnu_categories.txt" ;;
          esac
          if [ ! -f "$SRC" ]; then
            echo "Missing categories file: $SRC"; exit 2
          fi
          cp "$SRC" /tmp/categories.txt
          echo "path=/tmp/categories.txt" >> "$GITHUB_OUTPUT"

      # Presence-only check so we fail early if secret is missing
      - name: Preflight DB env (presence-only)
        env:
          DATABASE_URL: ${{ secrets.DATABASE_URL_PUBLIC }}
        run: |
          if [ -z "${DATABASE_URL:-}" ]; then
            echo "::warning::DATABASE_URL not set — upsert will be skipped."
          else
            echo "::add-mask::${DATABASE_URL}"
            echo "::notice::DATABASE_URL is present. Proceeding."
          fi

      # DNS + short asyncpg connect check to the same DATABASE_URL_PUBLIC
      - name: DB preflight (DNS + connect)
        env:
          DATABASE_URL: ${{ secrets.DATABASE_URL_PUBLIC }}
        run: |
          set -euo pipefail
          if [ -z "${DATABASE_URL:-}" ]; then
            echo "::notice::No DATABASE_URL provided; skipping DB preflight."
            exit 0
          fi
          python - <<'PY'
          import os, socket, sys, urllib.parse, asyncio
          url = os.environ['DATABASE_URL']
          p = urllib.parse.urlparse(url)
          host, port = p.hostname, p.port or 5432
          print(f"[preflight] host={host} port={port}")
          try:
              addr = socket.getaddrinfo(host, port)[0][4][0]
              print(f"[preflight] DNS {host} -> {addr}:{port}")
          except Exception as e:
              print(f"::error::DNS resolution failed for {host}:{port} -> {e}")
              sys.exit(1)
          async def go():
              try:
                  import asyncpg
              except Exception:
                  print("::warning::asyncpg not installed; skipping connect test")
                  return
              try:
                  conn = await asyncpg.connect(url, timeout=6)
                  ver = await conn.fetchval("select version()")
                  print("[preflight] connected ok:", ver.split()[0])
                  await conn.close()
              except Exception as e:
                  print(f"::error::DB connect failed: {e}")
                  sys.exit(1)
          asyncio.run(go())
          PY

      - name: Run Wolt crawler
        env:
          PYTHONUNBUFFERED: "1"
          DATABASE_URL: ${{ secrets.DATABASE_URL_PUBLIC }}
          COOP_UPSERT_DB: ${{ github.event_name == 'workflow_dispatch' && inputs.upsert_db || '1' }}
          COOP_DEDUP_DB: ${{ github.event_name == 'workflow_dispatch' && inputs.upsert_db || '1' }}
          # ⬇️ Force Playwright during testing (one-line swap)
          WOLT_FORCE_PLAYWRIGHT: "1"
          # Be gentle with prodinfo; keep modest by default
          WOLT_PROBE_LIMIT: "40"
          # Start without modal clicks while stabilizing; can raise later
          WOLT_MODAL_PROBE_LIMIT: "0"
          # Known venueIds (24-hex) per store
          VENUE_ID_PARN: "628210e98f6be4895159d8fe"      # Coop Pärnu Maksimarket (verified)
          VENUE_ID_LASNAMAE: "6282118813e5280bbe9e450f"   # Coop Lasnamäe (adjust if needed)
        run: |
          set -euo pipefail
          # small randomized stagger to avoid bursts / 429s when multiple stores
          SLEEP=$(( (RANDOM % 25) + 10 ))
          echo "Staggering start by ${SLEEP}s for store ${{ matrix.store }}..."
          sleep "${SLEEP}"

          MAX_PRODUCTS="${{ github.event_name == 'workflow_dispatch' && inputs.max_products || '0' }}"
          STORE_HOST="wolt:coop-${{ matrix.store }}"
          OUT="out/coop_wolt_${{ github.run_id }}_${{ matrix.store }}.csv"

          # pick venue id for this store
          case "${{ matrix.store }}" in
            parnu)   VENUE_ID="${VENUE_ID_PARN}" ;;
            lasname) VENUE_ID="${VENUE_ID_LASNAMAE}" ;;
            *)       VENUE_ID="" ;;
          esac

          echo "Running ${STORE_HOST} (venueId=${VENUE_ID:-auto}) → ${OUT}"
          stdbuf -oL -eL python3 scripts/wolt_crawler.py \
            --store-host "${STORE_HOST}" \
            --categories-file "${{ steps.cats.outputs.path }}" \
            --max-products "${MAX_PRODUCTS}" \
            --headless "1" \
            --req-delay "1.0" \
            --goto-strategy "domcontentloaded" \
            --nav-timeout "45000" \
            --category-timeout "210" \
            --upsert-per-category \
            --flush-every "120" \
            --probe-limit "${WOLT_PROBE_LIMIT}" \
            --modal-probe-limit "${WOLT_MODAL_PROBE_LIMIT}" \
            ${VENUE_ID:+--venue-id "${VENUE_ID}"} \
            --out "${OUT}"

      - name: Upload CSV artifact
        uses: actions/upload-artifact@v4
        with:
          name: coop-wolt-${{ matrix.store }}-${{ github.run_id }}
          path: out/*.csv
          if-no-files-found: warn
          retention-days: 7
