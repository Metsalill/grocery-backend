name: "Coop Wolt – Crawl (Pärnu + Tallinn venues)"

on:
  schedule:
    - cron: "0 */4 * * *"   # every 4 hours, at minute 0
  workflow_dispatch:
    inputs:
      store:
        description: "Which Wolt Coop venue(s) to crawl"
        required: false
        default: "parnu"
        type: choice
        options:
          - parnu
          - coop-lasname
          - coop-mustakivi
          - coop-akadeemia
          - coop-miiduranna
          - coop-laagri
          - konsum-juri
          - konsum-saku
          - all
      upsert_db:
        description: "1 = upsert into DATABASE_URL_PUBLIC (staging_coop_products)"
        required: false
        default: "0"

jobs:
  crawl:
    runs-on: ubuntu-latest
    timeout-minutes: 180
    outputs:
      stores: ${{ steps.decide.outputs.stores }}

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      # Decide which venue slug(s) to crawl -> JSON array for matrix
      - name: Decide store matrix
        id: decide
        shell: bash
        run: |
          set -euo pipefail
          # Default set (all Tallinn venues + Pärnu)
          ALL='["coop-prnu","coop-lasname","coop-mustakivi","coop-akadeemia","coop-miiduranna","coop-laagri","konsum-juri","konsum-saku"]'

          if [ "${{ github.event_name }}" != "workflow_dispatch" ]; then
            echo '["coop-prnu"]' > /tmp/stores.json   # scheduled: Pärnu only
          else
            case "${{ inputs.store }}" in
              all)                  echo "${ALL}"                > /tmp/stores.json ;;
              parnu)                echo '["coop-prnu"]'         > /tmp/stores.json ;;
              coop-lasname)         echo '["coop-lasname"]'      > /tmp/stores.json ;;
              coop-mustakivi)       echo '["coop-mustakivi"]'    > /tmp/stores.json ;;
              coop-akadeemia)       echo '["coop-akadeemia"]'    > /tmp/stores.json ;;
              coop-miiduranna)      echo '["coop-miiduranna"]'   > /tmp/stores.json ;;
              coop-laagri)          echo '["coop-laagri"]'       > /tmp/stores.json ;;
              konsum-juri)          echo '["konsum-juri"]'       > /tmp/stores.json ;;
              konsum-saku)          echo '["konsum-saku"]'       > /tmp/stores.json ;;
              *)                    echo '["coop-prnu"]'         > /tmp/stores.json ;;
            esac
          fi
          echo "stores=$(cat /tmp/stores.json)" >> "$GITHUB_OUTPUT"

  run-crawlers:
    runs-on: ubuntu-latest
    timeout-minutes: 180
    needs: crawl
    concurrency:
      group: coop-wolt-${{ matrix.store }}
      cancel-in-progress: false

    strategy:
      fail-fast: false
      max-parallel: 1
      matrix:
        store: ${{ fromJSON(needs.crawl.outputs.stores || '["coop-prnu"]') }}

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: System deps for Playwright
        shell: bash
        run: |
          sudo apt-get update
          sudo apt-get install -y libxml2-dev libxslt1-dev python3-dev build-essential libffi-dev libssl-dev libjpeg-dev

      - name: Install deps (pip + Playwright)
        shell: bash
        run: |
          python -m pip install --upgrade pip wheel
          if [ -f requirements.txt ]; then
            pip install -r requirements.txt
          fi
          pip install playwright selectolax lxml "psycopg[binary]>=3.1" asyncpg
          python -m playwright install --with-deps chromium

      - name: Prepare categories file (shared slugs-only list)
        id: cats
        shell: bash
        run: |
          set -euo pipefail
          mkdir -p out
          SRC="data/wolt_categories_slugs.txt"
          if [ ! -f "$SRC" ]; then
            echo "::error::Missing categories file: $SRC"
            exit 2
          fi
          cp "$SRC" /tmp/categories.txt
          echo "path=/tmp/categories.txt" >> "$GITHUB_OUTPUT"

      - name: Preflight DB env (presence-only)
        env:
          DATABASE_URL: ${{ secrets.DATABASE_URL_PUBLIC }}
        shell: bash
        run: |
          if [ -z "${DATABASE_URL:-}" ]; then
            echo "::warning::DATABASE_URL not set — upsert will be skipped."
          else
            echo "::add-mask::${DATABASE_URL}"
            echo "::notice::DATABASE_URL is present. Proceeding."
          fi

      - name: DB preflight (DNS + connect)
        env:
          DATABASE_URL: ${{ secrets.DATABASE_URL_PUBLIC }}
        shell: bash
        run: |
          set -euo pipefail
          if [ -z "${DATABASE_URL:-}" ]; then
            echo "::notice::No DATABASE_URL provided; skipping DB preflight."
            exit 0
          fi
          python - <<'PY'
          import os, socket, sys, urllib.parse, asyncio
          url = os.environ['DATABASE_URL']
          p = urllib.parse.urlparse(url)
          host, port = p.hostname, p.port or 5432
          print(f"[preflight] host={host} port={port}")
          try:
              addr = socket.getaddrinfo(host, port)[0][4][0]
              print(f"[preflight] DNS {host} -> {addr}:{port}")
          except Exception as e:
              print(f"::error::DNS resolution failed for {host}:{port} -> {e}")
              sys.exit(1)
          async def go():
              try:
                  import asyncpg
              except Exception:
                  print("::warning::asyncpg not installed; skipping connect test")
                  return
              try:
                  conn = await asyncpg.connect(url, timeout=6)
                  ver = await conn.fetchval("select version()")
                  print("[preflight] connected ok:", ver.split()[0])
                  await conn.close()
              except Exception as e:
                  print(f"::error::DB connect failed: {e}")
                  sys.exit(1)
          asyncio.run(go())
          PY

      - name: Run Wolt crawler
        env:
          PYTHONUNBUFFERED: "1"
          DATABASE_URL: ${{ secrets.DATABASE_URL_PUBLIC }}
        shell: bash
        run: |
          set -euo pipefail
          # stagger a bit when multiple venues
          SLEEP=$(( (RANDOM % 25) + 10 ))
          echo "Staggering start by ${SLEEP}s for venue ${{ matrix.store }}..."
          sleep "${SLEEP}"

          VENUE_SLUG="${{ matrix.store }}"
          STORE_HOST="wolt:${VENUE_SLUG}"
          # City hint: Pärnu vs Tallinn (affects headers/cookies only)
          if [ "${VENUE_SLUG}" = "coop-prnu" ]; then
            CITY="parnu"
          else
            CITY="tallinn"
          fi

          OUT="out/coop_wolt_${{ github.run_id }}_${VENUE_SLUG}.csv"
          UPSERT_FLAG="${{ github.event_name == 'workflow_dispatch' && inputs.upsert_db || '1' }}"

          echo "Running ${STORE_HOST} (city=${CITY}) → ${OUT}"
          stdbuf -oL -eL python3 scripts/wolt_crawler.py \
            --store-host "${STORE_HOST}" \
            --city "${CITY}" \
            --language "et" \
            --categories-file "${{ steps.cats.outputs.path }}" \
            --out "${OUT}" \
            --upsert-db "${UPSERT_FLAG}"

      - name: Upload CSV artifact (partial or final)
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: coop-wolt-${{ matrix.store }}-${{ github.run_id }}
          path: out/*.csv
          if-no-files-found: warn
          retention-days: 7
